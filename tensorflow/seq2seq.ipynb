{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "GO_TOKEN = 0\n",
    "END_TOKEN = 1\n",
    "UNK_TOKEN = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def seq2seq(mode, features, labels, params):\n",
    "    '''Model parameters'''\n",
    "    vocab_size = params['vocab_size'] # Embeding layer\n",
    "    embed_dim = params['embed_dim']   # Embeding layer \n",
    "    num_units = params['num_units']   # Rnn cell\n",
    "    input_max_length = params['input_max_length']\n",
    "    output_max_length = params['output_max_length']\n",
    "    \n",
    "    '''training data'''\n",
    "    inp = features['input']\n",
    "    output = features['output']\n",
    "    \n",
    "    '''Parameters of training processing'''\n",
    "    batch_size = tf.shape(inp)[0] # length of one row??\n",
    "    start_tokens = tf.zeros([batch_size], dtype=tf.int64)\n",
    "    train_output = tf.concat([tf.expand_dims(start_tokens, 1), output], 1)\n",
    "    # tf.expand_dims: Inserts a dimension of 1 into a tensor's shape.\n",
    "    input_lengths = tf.reduce_sum(tf.to_int32(tf.not_equal(inp, 1)), 1)\n",
    "    output_lengths = tf.reduce_sum(tf.to_int32(tf.not_equal(train_output, 1)), 1)\n",
    "    # tf.not_equal: Returns the truth value of (x != y) element-wise.\n",
    "    # tf.reduce_sum: Computes the sum of elements across dimensions of a tensor. \n",
    "    \n",
    "    '''Building the model: embeding layer''' \n",
    "    input_embed = layers.embed_sequence(\n",
    "                         inp, vocab_size=vocab_size, embed_dim=embed_dim, scope='embed')\n",
    "    output_embed = layers.embed_sequence(\n",
    "                          train_output, vocab_size=vocab_size, \n",
    "                          embed_dim=embed_dim, scope='embed', reuse=True)\n",
    "    # tf.contrib.layers.embed_sequence: Maps a sequence of symbols to a sequence of embeddings.\n",
    "    with tf.variable_scope('embed', reuse=True):\n",
    "        '''Sharing variable'''\n",
    "        embeddings = tf.get_variable('embeddings')\n",
    "    # tf.variable_scope: A context manager for defining ops that creates variables (layers)\n",
    "    # tf.get_variable: Gets an existing variable with these parameters or create a new one.\n",
    "\n",
    "    '''Building model: Rnn(GRU)'''\n",
    "    cell = tf.contrib.rnn.GRUCell(num_units=num_units)\n",
    "    encoder_outputs, encoder_final_state = tf.nn.dynamic_rnn(cell, input_embed, dtype=tf.float32)\n",
    "\n",
    "    train_helper = tf.contrib.seq2seq.TrainingHelper(output_embed, output_lengths)\n",
    "    # tf.contrib.seq2seq.TrainingHelper():\n",
    "    # A helper for use during training. Only reads inputs.\n",
    "    # Returned sample_ids are the argmax of the RNN output logits.\n",
    "    \n",
    "    # train_helper = tf.contrib.seq2seq.ScheduledEmbeddingTrainingHelper(\n",
    "    #     output_embed, output_lengths, embeddings, 0.3\n",
    "    # )\n",
    "    pred_helper = tf.contrib.seq2seq.GreedyEmbeddingHelper(\n",
    "                  embeddings, start_tokens=tf.to_int32(start_tokens), end_token=1)\n",
    "    # tf.contrib.seq2seq.GreedyEmbeddingHelper: A helper for use during inference.\n",
    "    # Uses the argmax of the output (treated as logits) and passes the result through \n",
    "    # an embedding layer to get the next input\n",
    "    \n",
    "\n",
    "    def decode(helper, scope, reuse=None):\n",
    "        with tf.variable_scope(scope, reuse=reuse):\n",
    "            attention_mechanism = tf.contrib.seq2seq.BahdanauAttention(\n",
    "                num_units=num_units, memory=encoder_outputs,\n",
    "                memory_sequence_length=input_lengths)\n",
    "            # tf.contrib.seq2seq.BahdanauAttention: Implements Bahdanau-style (additive) attention.\n",
    "            cell = tf.contrib.rnn.GRUCell(num_units=num_units)\n",
    "            attn_cell = tf.contrib.seq2seq.AttentionWrapper(\n",
    "                cell, attention_mechanism, attention_layer_size=num_units / 2)\n",
    "            # tf.contrib.seq2seq.AttentionWrapper: Wraps another RNNCell with attention.\n",
    "            out_cell = tf.contrib.rnn.OutputProjectionWrapper(\n",
    "                             attn_cell, vocab_size, reuse=reuse\n",
    "                             )\n",
    "            # tf.contrib.rnn.OutputProjectionWrapper: Operator adding an output projection to the given cell.\n",
    "            decoder = tf.contrib.seq2seq.BasicDecoder(\n",
    "                cell=out_cell, helper=helper,\n",
    "                initial_state=out_cell.zero_state(\n",
    "                    dtype=tf.float32, batch_size=batch_size))\n",
    "                #initial_state=encoder_final_state)\n",
    "            # tf.contrib.seq2seq.BasicDecoder: Basic sampling decoder.\n",
    "            outputs = tf.contrib.seq2seq.dynamic_decode(\n",
    "                                decoder=decoder, output_time_major=False,\n",
    "                                impute_finished=True, maximum_iterations=output_max_length\n",
    "                                )\n",
    "            # tf.contrib.seq2seq.dynamic_decode: Perform dynamic decoding with decoder.\n",
    "            return outputs[0]\n",
    "        \n",
    "        \n",
    "    train_outputs = decode(train_helper, 'decode')\n",
    "    pred_outputs = decode(pred_helper, 'decode', reuse=True)\n",
    "\n",
    "    tf.identity(train_outputs.sample_id[0], name='train_pred')\n",
    "    weights = tf.to_float(tf.not_equal(train_output[:, :-1], 1))\n",
    "    loss = tf.contrib.seq2seq.sequence_loss(\n",
    "                      train_outputs.rnn_output, output, weights=weights)\n",
    "    train_op = layers.optimize_loss(\n",
    "                      loss, tf.train.get_global_step(),\n",
    "                      optimizer=params.get('optimizer', 'Adam'),\n",
    "                      learning_rate=params.get('learning_rate', 0.001),\n",
    "                      summaries=['loss', 'learning_rate'])\n",
    "\n",
    "    tf.identity(pred_outputs.sample_id[0], name='predictions')\n",
    "    return tf.estimator.EstimatorSpec(\n",
    "                        mode=mode,\n",
    "                        predictions=pred_outputs.sample_id,\n",
    "                        loss=loss,\n",
    "                        train_op=train_op)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tokenize_and_map(line, vocab):\n",
    "    return [vocab.get(token, UNK_TOKEN) for token in line.split(' ')]\n",
    "\n",
    "\n",
    "''' Create function to reads data and generates padded pairs of (input, output) of equal lenght'''\n",
    "def make_input_fn(\n",
    "        batch_size, input_filename, output_filename, vocab,\n",
    "        input_max_length, output_max_length,\n",
    "        input_process=tokenize_and_map, output_process=tokenize_and_map):\n",
    "\n",
    "    def input_fn():\n",
    "        inp = tf.placeholder(tf.int64, shape=[None, None], name='input')\n",
    "        output = tf.placeholder(tf.int64, shape=[None, None], name='output')\n",
    "        tf.identity(inp[0], 'input_0')\n",
    "        tf.identity(output[0], 'output_0')\n",
    "        return { 'input': inp,\n",
    "                 'output': output,}, None\n",
    "\n",
    "    def sampler():\n",
    "        while True:\n",
    "            with open(input_filename) as finput:\n",
    "                with open(output_filename) as foutput:\n",
    "                    for in_line in finput:\n",
    "                        out_line = foutput.readline()\n",
    "                        yield {\n",
    "                            'input': input_process(in_line, vocab)[:input_max_length - 1] + [END_TOKEN],\n",
    "                            'output': output_process(out_line, vocab)[:output_max_length - 1] + [END_TOKEN]\n",
    "                              }\n",
    "\n",
    "    sample_me = sampler()\n",
    "\n",
    "    def feed_fn():\n",
    "        inputs, outputs = [], []\n",
    "        input_length, output_length = 0, 0\n",
    "        for i in range(batch_size):\n",
    "            rec = sample_me.next()\n",
    "            inputs.append(rec['input'])\n",
    "            outputs.append(rec['output'])\n",
    "            input_length = max(input_length, len(inputs[-1]))\n",
    "            output_length = max(output_length, len(outputs[-1]))\n",
    "        # Pad me right with </S> token.\n",
    "        for i in range(batch_size):\n",
    "            inputs[i] += [END_TOKEN] * (input_length - len(inputs[i]))\n",
    "            outputs[i] += [END_TOKEN] * (output_length - len(outputs[i]))\n",
    "        return { 'input:0': inputs,\n",
    "                 'output:0': outputs }\n",
    "    \n",
    "    return input_fn, feed_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_rev_vocab(vocab):\n",
    "    return {idx: key for key, idx in vocab.iteritems()}\n",
    "\n",
    "\n",
    "def get_formatter(keys, vocab):\n",
    "    rev_vocab = get_rev_vocab(vocab)\n",
    "\n",
    "    def to_str(sequence):\n",
    "        tokens = [\n",
    "            rev_vocab.get(x, \"<UNK>\") for x in sequence]\n",
    "        return ' '.join(tokens)\n",
    "\n",
    "    def format(values):\n",
    "        res = []\n",
    "        for key in keys:\n",
    "            res.append(\"%s = %s\" % (key, to_str(values[key])))\n",
    "        return '\\n'.join(res)\n",
    "    return format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_vocab(filename):\n",
    "    vocab = {}\n",
    "    with open(filename) as f:\n",
    "        for idx, line in enumerate(f):\n",
    "            vocab[line.strip()] = idx\n",
    "            # strip(): returns a copy of the string \n",
    "            #         (all chars have been stripped from the beginning and the end)\n",
    "    return vocab\n",
    "\n",
    "\n",
    "def train_seq2seq( input_filename, output_filename, \n",
    "                   vocab_filename, model_dir ):\n",
    "    \n",
    "    vocab = load_vocab(vocab_filename) # call load_vocab function, return a dict\n",
    "    params = {\n",
    "              'vocab_size': len(vocab),\n",
    "              'batch_size': 32,\n",
    "              'input_max_length': 30,\n",
    "              'output_max_length': 30,\n",
    "              'embed_dim': 100,\n",
    "              'num_units': 256\n",
    "                }\n",
    "    \n",
    "    est = tf.estimator.Estimator(model_fn=seq2seq,\n",
    "                                 model_dir=model_dir, # 'model/seq2seq'\n",
    "                                 params=params )\n",
    "    # tf.estimator.Estimator: a class to train and evaluate TensorFlow models.\n",
    "\n",
    "    input_fn, feed_fn = make_input_fn( params['batch_size'],\n",
    "                                       input_filename,\n",
    "                                       output_filename,\n",
    "                                       vocab, params['input_max_length'], \n",
    "                                       arams['output_max_length'] )\n",
    "\n",
    "    # Make hooks to print examples of inputs/predictions.\n",
    "    print_inputs = tf.train.LoggingTensorHook(\n",
    "                             ['input_0', 'output_0'], every_n_iter=100,\n",
    "                             formatter=get_formatter(['input_0', 'output_0'], vocab))\n",
    "    print_predictions = tf.train.LoggingTensorHook(\n",
    "                             ['predictions', 'train_pred'], every_n_iter=100,\n",
    "                             formatter=get_formatter(['predictions', 'train_pred'], vocab))\n",
    "    #tf.train.LoggingTensorhook: Prints the given tensors every N local steps, every N seconds, or at end\n",
    "    timeline_hook = timeline.TimelineHook(model_dir, every_n_iter=100)\n",
    "    \n",
    "    \n",
    "    est.train( input_fn=input_fn,\n",
    "               hooks=[ tf.train.FeedFnHook(feed_fn), \n",
    "                       print_inputs, \n",
    "                       print_predictions,\n",
    "                       timeline_hook ],\n",
    "               steps=10000)\n",
    "    # tf.train.FeedFnHook: Runs feed_fn and sets the feed_dict accordingly.\n",
    "\n",
    "\n",
    "def main():\n",
    "    tf.logging._logger.setLevel(logging.INFO)\n",
    "    train_seq2seq('input', 'output', 'vocab', 'model/seq2seq')\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
