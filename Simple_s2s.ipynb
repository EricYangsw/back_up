{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow Version 1.8.0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "#from tensorflow.contrib.rnn import GRUCell\n",
    "from tensorflow.python.layers import core as layers_core\n",
    "\n",
    "# __all__ = [\"AttentionModel\"]\n",
    "print(\"TensorFlow Version\", tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"tensorflow.python.layers:\n",
    "layers 模塊提供用於深度學習的更高層次封裝的 API，\n",
    "\n",
    "tf.layers 模塊提供的方法有：\n",
    "    Input(…): 用於實例化一個輸入 Tensor，作為神經網絡的輸入。\n",
    "    average_pooling1d(…): 一維平均池化層\n",
    "    average_pooling2d(…): 二維平均池化層\n",
    "    average_pooling3d(…): 三維平均池化層\n",
    "    batch_normalization(…): 批量標準化層\n",
    "    conv1d(…): 一維卷積層\n",
    "    conv2d(…): 二維卷積層\n",
    "    conv2d_transpose(…): 二維反捲積層\n",
    "    conv3d(…): 三維卷積層\n",
    "    conv3d_transpose(…): 三維反捲積層\n",
    "    dense(…): 全連接層\n",
    "    dropout(…): Dropout層\n",
    "    flatten(…): Flatten層，即把一個 Tensor 展平\n",
    "    max_pooling1d(…): 一維最大池化層\n",
    "    max_pooling2d(…): 二維最大池化層\n",
    "    max_pooling3d(…): 三維最大池化層\n",
    "    separable_conv2d(…): 二維深度可分離卷積層\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_input = pd.read_csv('plc_x_reduce.csv')\n",
    "y_label = pd.read_csv('plc_y_reduce.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_generator(x_input,\n",
    "                    y_label,\n",
    "                    batch_size=1, \n",
    "                    seq_len=1):\n",
    "    \n",
    "    batchs = int(y_label.shape[0] / seq_len / batch_size)\n",
    "    print(batchs)\n",
    "    \n",
    "    input_data = []\n",
    "    target_data = []\n",
    "    \n",
    "    \n",
    "    for i in range(batchs):\n",
    "        x = np.zeros(shape=(batch_size, seq_len, x_input.shape[1]))\n",
    "        y = np.zeros(shape=(batch_size, seq_len, y_label.shape[1]))\n",
    "        for b in range(batch_size):\n",
    "\n",
    "            x[b, :, :] = x_input[b*i : b*i+seq_len, :]\n",
    "            y[b, :, :] = y_label[b*i : b*i+seq_len, :]\n",
    "\n",
    "        input_data.append(x)\n",
    "        target_data.append(y)              \n",
    "    return input_data, target_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2Seq:\n",
    "    def __init__(self,\n",
    "                 seq_max_len=1.,  \n",
    "                 input_len=71.,\n",
    "                 output_len=41.,\n",
    "                 batch_size=1,\n",
    "                 lstm_size=[71., 80., 41.],\n",
    "                 num_layers=3.,\n",
    "                 learning_rate=0.001,\n",
    "                 grad_clip=2.,\n",
    "                 keep_prob=1.,\n",
    "                 forward_only= None):\n",
    "        \n",
    "        \n",
    "        self.seq_max_len = seq_max_len\n",
    "        self.batch_size = batch_size\n",
    "        self.batch_len = np.array([])\n",
    "        self.input_len = input_len\n",
    "        self.output_len = output_len\n",
    "        self.lstm_size = lstm_size\n",
    "        self.num_units = self.lstm_size[-1]\n",
    "        self.num_layers = num_layers\n",
    "        self.learning_rate = learning_rate\n",
    "        self.grad_clip = grad_clip\n",
    "        self.train_keep_prob = keep_prob\n",
    "        self.decoder_hidden_size = lstm_size[-1]\n",
    "        \n",
    "        self.batch_seq_len = np.int32(np.ones(shape=([self.batch_size])) * self.seq_max_len)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        tf.reset_default_graph() #Clears the default graph stack and resets the global default graph\n",
    "        self.build_inputs()\n",
    "        #self.build_embedding()\n",
    "        self.build_encoder()\n",
    "        self.build_atten_decoder()\n",
    "        self.build_loss()\n",
    "        self.build_optimizer()\n",
    "        self.saver = tf.train.Saver() #Saves and restores variables.\n",
    " \n",
    "        \n",
    "\n",
    "    def build_inputs(self):\n",
    "        self.encoder_inputs = tf.placeholder(tf.float32, \n",
    "                                         shape=(self.batch_size, self.seq_max_len, self.input_len),\n",
    "                                         name='inputs')\n",
    "\n",
    "        self.targets = tf.placeholder(tf.float32,\n",
    "                                          shape=(self.batch_size, self.seq_max_len, self.output_len),\n",
    "                                          name='targets')\n",
    "\n",
    "        self.keep_prob = tf.placeholder(tf.float32, name='keep_prob')\n",
    "        \n",
    "        self.decoder_inputs = tf.placeholder(tf.float32,\n",
    "                                                shape=(self.batch_size, self.seq_max_len, self.lstm_size[-1]),  \n",
    "                                                name='decoder_inputs')\n",
    "\n",
    "        \n",
    "        #self.input_sequence_length = tf.placeholder(shape=([self.batch_size]), dtype=tf.int32, name='input_length')\n",
    "        #self.decoder_sequence_length = tf.placeholder(shape=([self.batch_size]), dtype=tf.int32, name='decoder_inputs_length')\n",
    "        #self.target_sequence_length = tf.placeholder(shape=([self.batch_size]), dtype=tf.float32, name='target_sequence_length')\n",
    "\n",
    "\n",
    "    '''\n",
    "    def build_embedding(self):\n",
    "        # Embedding\n",
    "        initializer = tf.random_uniform_initializer(-1, 1, dtype=tf.float32)\n",
    "        embeddings = tf.get_variable(name='embedding',\n",
    "                                     shape=[vocab_size, encoder_embedding_size],\n",
    "                                     initializer=initializer, \n",
    "                                     dtype=tf.float32)\n",
    "        # encoder_inputs_embedded = tf.nn.embedding_lookup(embeddings, self.encoder_inputs)\n",
    "        self.decoder_inputs_embedded = tf.nn.embedding_lookup(embeddings, self.decoder_inputs)\n",
    "    '''\n",
    "\n",
    "    def build_encoder(self):\n",
    "        def get_a_cell(lstm_size, keep_prop):\n",
    "            lstm = tf.nn.rnn_cell.BasicLSTMCell(lstm_size)\n",
    "            drop = tf.nn.rnn_cell.DropoutWrapper(lstm, output_keep_prob=self.train_keep_prob)\n",
    "            return drop\n",
    "\n",
    "        with tf.variable_scope('encoder', initializer=tf.orthogonal_initializer()):\n",
    "            encoder_cell = tf.nn.rnn_cell.MultiRNNCell(\n",
    "                                 [get_a_cell(size, self.keep_prob) for size in self.lstm_size]\n",
    "                                                      )\n",
    "            self.initial_state = encoder_cell.zero_state(self.batch_size, tf.float32)\n",
    "            # 透過dynamic_rnn對cell展開時間維度\n",
    "            self.encoder_outputs  = tf.nn.dynamic_rnn(\n",
    "                                                      encoder_cell, \n",
    "                                                      self.encoder_inputs,                                                    \n",
    "                                                      initial_state=self.initial_state\n",
    "                                                      )\n",
    "\n",
    "\n",
    "    # Decoder with Attention ----------------------------------------------------------------------\n",
    "    def build_atten_decoder(self):\n",
    "        decoder_cell = tf.nn.rnn_cell.BasicLSTMCell(self.lstm_size[-1])\n",
    "        #decoder_cell = tf.contrib.rnn.GRUCell(self.lstm_size[-1])\n",
    "        \n",
    "\n",
    "\n",
    "        # Training Helper\n",
    "        helper = tf.contrib.seq2seq.TrainingHelper(\n",
    "                    self.decoder_inputs, self.batch_seq_len, time_major=True)\n",
    "\n",
    "\n",
    "        # Project layer (full connecting layers)\n",
    "        project_layer = layers_core.Dense(self.output_len , use_bias=False, name=\"output_projection\")\n",
    "        \n",
    "        self.d_initial_state = decoder_cell.zero_state(self.batch_size, tf.float32)\n",
    "        decoder = tf.contrib.seq2seq.BasicDecoder(\n",
    "                                           cell=decoder_cell,\n",
    "                                           helper=helper,\n",
    "                                           initial_state=self.d_initial_state,\n",
    "                                           output_layer=project_layer\n",
    "                                                 )\n",
    "\n",
    "        # Attention\n",
    "        #attenetion_states: [batch_size, max_time, num_units]\n",
    "        #attention_states = tf.transpose(self.encoder_outputs, [1, 0, 2]) #why?\n",
    "        '''\n",
    "        attention_mechanism = tf.contrib.seq2seq.LuongAttention(\n",
    "                                          num_units = self.num_units, \n",
    "                                          memory=self.encoder_outputs)\n",
    "\n",
    "        decoder_cell = tf.contrib.seq2seq.AttentionWrapper(\n",
    "                                          decoder_cell,\n",
    "                                          attention_mechanism,         \n",
    "                                          alignment_history=False,\n",
    "                                          cell_input_fn=None,\n",
    "                                          output_attention=True,\n",
    "                                          initial_cell_state=None,\n",
    "                                          name=None)\n",
    "        '''\n",
    "\n",
    "        # Outout\n",
    "        final_outputs, _, _  = tf.contrib.seq2seq.dynamic_decode(decoder)\n",
    "        self.logits = final_outputs.rnn_output\n",
    "        self.proba_prediction = tf.tanh(self.logits, name='predictions')\n",
    "\n",
    "\n",
    "    def build_loss(self):\n",
    "        with tf.name_scope('loss'):\n",
    "            #self.y_reshaped = tf.reshape(self.targets,  self.logits.get_shape())\n",
    "            loss =tf.losses.mean_squared_error(predictions=self.proba_prediction, labels=self.targets)\n",
    "            #loss = tf.nn.sigmoid_cross_entropy_with_logits(logits=self.logits, labels=self.y_reshaped)\n",
    "            self.loss = tf.reduce_mean(loss)\n",
    "\n",
    "\n",
    "\n",
    "    def build_optimizer(self):\n",
    "        # Using \"clipping\" gradients\n",
    "        tvars = tf.trainable_variables()\n",
    "        grads, _ = tf.clip_by_global_norm(tf.gradients(self.loss, tvars), self.grad_clip)\n",
    "        train_op = tf.train.AdamOptimizer(self.learning_rate)\n",
    "        self.optimizer = train_op.apply_gradients(zip(grads, tvars))\n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "    # Training===============================================================================    \n",
    "    def train(self, x, y, iters=10, save_path='./models', save_every_n=200, log_every_n=200):\n",
    "        self.session = tf.Session()\n",
    "        with self.session as sess:\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "            # Train network\n",
    "\n",
    "            new_state = sess.run(self.initial_state)\n",
    "            for ite in range(iters):\n",
    "                step = 0\n",
    "                print('iters',ite)\n",
    "                for i in range(len(x)):\n",
    "                    step += 1\n",
    "                    start = time.time()\n",
    "                    \n",
    "                    feed = {self.encoder_inputs: x[i], \n",
    "                            self.targets: y[i],\n",
    "                            self.decoder_inputs : y[i],\n",
    "                            self.keep_prob: self.train_keep_prob,\n",
    "                            self.initial_state: new_state}\n",
    "                    \n",
    "                    batch_loss, new_state = sess.run([self.loss,\n",
    "                                                      self.optimizer],\n",
    "                                                      feed_dict=feed)\n",
    "                    # print result\n",
    "                    #self.print_result(x[i], y[i])\n",
    "                    end = time.time()\n",
    "\n",
    "                    # control the print lines\n",
    "                    if step % log_every_n == 0:\n",
    "                        print(\"=======================================================\\n\")\n",
    "                        print('step: {} in iter: {}/{}... '.format(step, ite+1, iters),\n",
    "                              'loss: {:.4f}... '.format(batch_loss),\n",
    "                              '{:.4f} sec/batch'.format((end - start)))\n",
    "\n",
    "                    if (step % save_every_n == 0):\n",
    "                        self.saver.save(sess, os.path.join(save_path, 'model'), global_step=step)\n",
    "                        #self.jodge(pred, target)\n",
    "                        #print(\"Target: \\n\", target)\n",
    "                        #print(\"PRED: \\n\",pred)\n",
    "\n",
    "\n",
    "    def jodge(self, pred, target):\n",
    "        pred = np.array(pred)\n",
    "        pred = np.array(pred >= 0.5).astype(int)\n",
    "\n",
    "        result = np.abs(pred - target)\n",
    "\n",
    "        for i in range(result.shape[0]):\n",
    "            np.savetxt(\"result_\" + str(i) + \".csv\", result[i], delimiter=',')\n",
    "        return pred            \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "52999\n",
      "iters 0\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Could not flatten dictionary. Key had 6 elements, but value had 1 elements. Key: [<tf.Tensor 'encoder/MultiRNNCellZeroState/DropoutWrapperZeroState/BasicLSTMCellZeroState/zeros:0' shape=(1, 71) dtype=float32>, <tf.Tensor 'encoder/MultiRNNCellZeroState/DropoutWrapperZeroState/BasicLSTMCellZeroState/zeros_1:0' shape=(1, 71) dtype=float32>, <tf.Tensor 'encoder/MultiRNNCellZeroState/DropoutWrapperZeroState_1/BasicLSTMCellZeroState/zeros:0' shape=(1, 80) dtype=float32>, <tf.Tensor 'encoder/MultiRNNCellZeroState/DropoutWrapperZeroState_1/BasicLSTMCellZeroState/zeros_1:0' shape=(1, 80) dtype=float32>, <tf.Tensor 'encoder/MultiRNNCellZeroState/DropoutWrapperZeroState_2/BasicLSTMCellZeroState/zeros:0' shape=(1, 41) dtype=float32>, <tf.Tensor 'encoder/MultiRNNCellZeroState/DropoutWrapperZeroState_2/BasicLSTMCellZeroState/zeros_1:0' shape=(1, 41) dtype=float32>], value: [None].",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-81-f263b6ebb401>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      9\u001b[0m             \u001b[0msave_path\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m             \u001b[0msave_every_n\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1000\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m             \u001b[0mlog_every_n\u001b[0m \u001b[1;33m=\u001b[0m\u001b[1;36m200\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m             )\n",
      "\u001b[1;32m<ipython-input-80-e78ab64bc125>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, x, y, iters, save_path, save_every_n, log_every_n)\u001b[0m\n\u001b[0;32m    183\u001b[0m                     batch_loss, new_state = sess.run([self.loss,\n\u001b[0;32m    184\u001b[0m                                                                      self.optimizer],\n\u001b[1;32m--> 185\u001b[1;33m                                                                      feed_dict=feed)\n\u001b[0m\u001b[0;32m    186\u001b[0m                     \u001b[1;31m# print result\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    187\u001b[0m                     \u001b[1;31m#self.print_result(x[i], y[i])\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\tf_cpu\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    898\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    899\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 900\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    901\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    902\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\tf_cpu\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1068\u001b[0m     \u001b[0mfeed_handles\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1069\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1070\u001b[1;33m       \u001b[0mfeed_dict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflatten_dict_items\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1071\u001b[0m       \u001b[1;32mfor\u001b[0m \u001b[0mfeed\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_val\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1072\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0msubfeed\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msubfeed_val\u001b[0m \u001b[1;32min\u001b[0m \u001b[0m_feed_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeed\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_val\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\tf_cpu\\lib\\site-packages\\tensorflow\\python\\util\\nest.py\u001b[0m in \u001b[0;36mflatten_dict_items\u001b[1;34m(dictionary)\u001b[0m\n\u001b[0;32m    230\u001b[0m             \u001b[1;34m\"Could not flatten dictionary. Key had %d elements, but value had \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    231\u001b[0m             \u001b[1;34m\"%d elements. Key: %s, value: %s.\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 232\u001b[1;33m             % (len(flat_i), len(flat_v), flat_i, flat_v))\n\u001b[0m\u001b[0;32m    233\u001b[0m       \u001b[1;32mfor\u001b[0m \u001b[0mnew_i\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnew_v\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mflat_i\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mflat_v\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    234\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mnew_i\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mflat_dictionary\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Could not flatten dictionary. Key had 6 elements, but value had 1 elements. Key: [<tf.Tensor 'encoder/MultiRNNCellZeroState/DropoutWrapperZeroState/BasicLSTMCellZeroState/zeros:0' shape=(1, 71) dtype=float32>, <tf.Tensor 'encoder/MultiRNNCellZeroState/DropoutWrapperZeroState/BasicLSTMCellZeroState/zeros_1:0' shape=(1, 71) dtype=float32>, <tf.Tensor 'encoder/MultiRNNCellZeroState/DropoutWrapperZeroState_1/BasicLSTMCellZeroState/zeros:0' shape=(1, 80) dtype=float32>, <tf.Tensor 'encoder/MultiRNNCellZeroState/DropoutWrapperZeroState_1/BasicLSTMCellZeroState/zeros_1:0' shape=(1, 80) dtype=float32>, <tf.Tensor 'encoder/MultiRNNCellZeroState/DropoutWrapperZeroState_2/BasicLSTMCellZeroState/zeros:0' shape=(1, 41) dtype=float32>, <tf.Tensor 'encoder/MultiRNNCellZeroState/DropoutWrapperZeroState_2/BasicLSTMCellZeroState/zeros_1:0' shape=(1, 41) dtype=float32>], value: [None]."
     ]
    }
   ],
   "source": [
    "model_path = './seq2seq_models'\n",
    "if os.path.exists(model_path) is False:\n",
    "    os.makedirs(model_path)\n",
    "input_data, target_data = batch_generator(x_input.values, y_label.values)\n",
    "model = Seq2Seq()\n",
    "model.train(input_data, \n",
    "            target_data,\n",
    "            iters=2,\n",
    "            save_path=model_path,\n",
    "            save_every_n=1000,\n",
    "            log_every_n =200\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "epochs = 50\n",
    "batch_size = 100\n",
    "learning_rate = 1e-3\n",
    "grad_clip = 0.3\n",
    "\n",
    "# Model parameter \n",
    "encoder_embedding_size = 128\n",
    "decoder_hidden_size = encoder_output_size = 128\n",
    "\n",
    "\n",
    "\n",
    "class trainer_class():\n",
    "    def train(self, model, data_transformer):\n",
    "        #Define some parameter and the optimizer here \n",
    "        n_iter = 0\n",
    "        tvars = tf.trainable_variables()\n",
    "        grads, _ = tf.clip_by_global_norm(tf.gradients(model.train_loss, tvars), grad_clip)\n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "        train_op = optimizer.apply_gradients(zip(grads, tvars))\n",
    "\n",
    "        with tf.Session() as sess:\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "            for epoch in range(0, epochs):\n",
    "                input_batches, target_batches = data_transformer.mini_batches(batch_size=batch_size)\n",
    "                for input_batch, target_batch in zip(input_batches, target_batches):    \n",
    "                    input_length = np.array(input_batch[1])\n",
    "                    target_length = np.array(target_batch[1])\n",
    "                    input_batch = np.transpose(np.array(input_batch[0]))\n",
    "                    target_batch = np.transpose(np.array(target_batch[0]))\n",
    "                    n_iter +=1\n",
    "                \n",
    "                    if target_length.shape[0] == batch_size:\n",
    "                        #Feed the batch into the training model \n",
    "                        output, loss, _ = sess.run([model.decoder_logits,model.train_loss,train_op],feed_dict={\n",
    "                                model.encoder_inputs: input_batch,\n",
    "                                model.decoder_targets: target_batch,\n",
    "                                model.decoder_inputs: target_batch,\n",
    "                                model.input_sequence_length: input_length,\n",
    "                                model.decoder_sequence_length: target_length,\n",
    "                                model.target_sequence_length: target_length\n",
    "                            })\n",
    "                        #Print loss and result after training 50 epoch\n",
    "                        if n_iter % 50 == 0:\n",
    "                            Predict_Words =[]\n",
    "                            Input_words = []\n",
    "                            # Two for loops are used to discard the \"EOS\"\n",
    "                            for batch in output:\n",
    "                                word = []\n",
    "                                for prob in batch:\n",
    "                                    Max=np.argmax(prob)\n",
    "                                    if Max ==1 or Max ==2:\n",
    "                                        continue\n",
    "                                    char = data_transformer.vocab.idx2char[Max]\n",
    "                                    word.append(char)\n",
    "                                    tmp_word = ''.join(word)\n",
    "                                Predict_Words.append(tmp_word)\n",
    "                            for batch in np.transpose(input_batch):\n",
    "                                word= data_transformer.vocab.indices_to_sequence(batch)\n",
    "                                Input_words.append(word)\n",
    "                            \n",
    "                            print(\"-----{}epochs------------- \".format(n_iter))\n",
    "                            print(\"loss: \",loss,\"\\n\")\n",
    "                            print(\"Input: \",Input_words,\"\\n\")\n",
    "                            print(\"Predict: \",Predict_Words,\"\")\n",
    "                            print(\"--------------------------\\n \")\n",
    "\n",
    "                        \n",
    "\n",
    "def main():\n",
    "    data_transformer = DataTransformer('../dataset/Google-10000-English.txt', use_cuda=False)\n",
    "    seq2seq_model = Model(encoder_embedding_size = encoder_embedding_size, \n",
    "                          encoder_output_size = encoder_output_size,\n",
    "                          batch_size = batch_size,\n",
    "                          vocab_size= data_transformer.vocab_size)\n",
    "    trainer = trainer_class()\n",
    "    trainer.train(seq2seq_model, data_transformer)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n",
    "\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
