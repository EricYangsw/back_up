{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Read config file name: ./config\n",
       "{\n",
       "    \"data\": {\n",
       "        \"type\": \"all\",\n",
       "        \"base_path\": \"data/\",\n",
       "        \"processed_path\": \"processed_all_dialogs_data\",\n",
       "        \"word_threshold\": 6,\n",
       "        \"max_seq_length\": 200,\n",
       "        \"sentence_diff\": 0.33,\n",
       "        \"testset_size\": 50000,\n",
       "        \"PAD_ID\": 0,\n",
       "        \"UNK_ID\": 1,\n",
       "        \"START_ID\": 2,\n",
       "        \"EOS_ID\": 3\n",
       "    },\n",
       "    \"model\": {\n",
       "        \"batch_size\": 32,\n",
       "        \"num_layers\": 4,\n",
       "        \"num_units\": 512,\n",
       "        \"embed_dim\": 300,\n",
       "        \"embed_share\": true,\n",
       "        \"cell_type\": \"gru\",\n",
       "        \"dropout\": 0.2,\n",
       "        \"encoder_type\": \"bi\",\n",
       "        \"attention_mechanism\": \"bahdanau\"\n",
       "    },\n",
       "    \"train\": {\n",
       "        \"learning_rate\": 0.001,\n",
       "        \"sampling_probability\": 0.4,\n",
       "        \"train_steps\": 100000,\n",
       "        \"model_dir\": \"logs/all_dialogs\",\n",
       "        \"save_checkpoints_steps\": 1000,\n",
       "        \"loss_hook_n_iter\": 1000,\n",
       "        \"check_hook_n_iter\": 1000,\n",
       "        \"min_eval_frequency\": 1000,\n",
       "        \"print_verbose\": true,\n",
       "        \"debug\": false\n",
       "    },\n",
       "    \"predict\": {\n",
       "        \"beam_width\": 0,\n",
       "        \"length_penalty_weight\": 1.0\n",
       "    },\n",
       "    \"slack\": {\n",
       "        \"webhook_url\": \"\"\n",
       "    }\n",
       "}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from hbconfig import Config\n",
    "Config(\"./config.yml\")\n",
    "Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "#__all__ = [\"Encoder\"]\n",
    "\n",
    "class Encoder:\n",
    "    \"\"\"Encoder class is Mutil-layer Recurrent Neural Networks\n",
    "    The 'Encoder' usually encode the sequential input vector.\n",
    "    \"\"\"\n",
    "    \n",
    "    UNI_ENCODER_TYPE = \"uni\"\n",
    "    BI_ENCODER_TYPE = \"bi\"\n",
    "    \n",
    "    RNN_GRU_CELL = \"gru\"\n",
    "    RNN_LSTM_CELL = \"lstm\"\n",
    "    RNN_NAS_CELL = \"nas\"\n",
    "    RNN_LAYER_NORM_LSTM_CELL = \"layer_norm_lstm\"\n",
    "    \n",
    "    def __init__(self, \n",
    "                 encoder_type=\"uni\",\n",
    "                 num_layers=4,\n",
    "                 cell_type=\"GRU\",\n",
    "                 num_units=512,\n",
    "                 dropout=0.8,\n",
    "                 dtype=tf.float32):\n",
    "        \"\"\"Contructs an 'Encoder' instance.\n",
    "        * Args:\n",
    "            encoder_type: rnn encoder_type (uni, bi)\n",
    "            num_layers: number of RNN cell composed sequentially of multiple simple cells.\n",
    "            input_vector: RNN Input vectors.\n",
    "            sequence_length: batch element's sequence length\n",
    "            cell_type: RNN cell types (lstm, gru, layer_norm_lstm, nas)\n",
    "            num_units: the number of units in cell\n",
    "            dropout: set prob operator adding dropout to inputs of the given cell.\n",
    "            dtype: the dtype of the input\n",
    "        * Returns:\n",
    "            Encoder instance\n",
    "        \"\"\"\n",
    "        self.encoder_type = encoder_type\n",
    "        self.num_layers = num_layers\n",
    "        self.cell_type = cell_type\n",
    "        self.num_units = num_units\n",
    "        self.dropout = dropout\n",
    "        self.dtype = dtype\n",
    "        \n",
    "    def build(self, input_vector, sequence_length, scope=None):\n",
    "        if self.encoder_type == self.UNIT_ENCODER_TYPE:\n",
    "            self.cells = self._create_rnn_clls()\n",
    "            return self.unidirectional_rnn(input_vector, sequence_length, scope=scope)\n",
    "        \n",
    "        elif self.encoder_type == self.BI_ENCODER_TYPE:\n",
    "            if self.num_layers == 0:\n",
    "                self.num_layers = 1\n",
    "            self.cells_fw = self._create_rnn_cells(is_list=True)\n",
    "            self.cells_bw = self._create_rnn_cells(is_list=True)\n",
    "            \n",
    "    def _create_rnn_cells(self, is_list=False):\n",
    "        \"\"\"Contructs stacked_rnn with num_layers\n",
    "        * Args:\n",
    "            is_list: flags for stack bidirectional. True=stack bidirectional, False=unidirectional\n",
    "        * Returns:\n",
    "            stacked_rnn\n",
    "        \"\"\"\n",
    "        stacked_rnn = []\n",
    "        for _ in range(self.num_layers):\n",
    "            single_cell = self._rnn_single_cell()\n",
    "            stacked_rnn.append(single_cell)\n",
    "            \n",
    "        if is_list:\n",
    "            return stacked_rnn\n",
    "        else:\n",
    "            return tf.nn.rnn_cell.MultiRNNCell(\n",
    "                                  cells=stacked_rnn,\n",
    "                                  state_is_tuple=True)\n",
    "        \n",
    "        \n",
    "    def _rnn_single_cell(self):\n",
    "        \"\"\"Contructs rnn single_cell\"\"\"\n",
    "\n",
    "        if self.cell_type == self.RNN_GRU_CELL:\n",
    "            single_cell = tf.contrib.rnn.GRUCell(\n",
    "                self.num_units,\n",
    "                reuse=tf.get_variable_scope().reuse)\n",
    "        elif self.cell_type == self.RNN_LSTM_CELL:\n",
    "            single_cell = tf.contrib.rnn.BasicLSTMCell(\n",
    "                self.num_units,\n",
    "                forget_bias=1.0,\n",
    "                reuse=tf.get_variable_scope().reuse)\n",
    "        elif self.cell_type == self.RNN_LAYER_NORM_LSTM_CELL:\n",
    "            single_cell = tf.contrib.rnn.LayerNormBasicLSTMCell(\n",
    "                self.num_units,\n",
    "                forget_bias=1.0,\n",
    "                layer_norm=True,\n",
    "                reuse=tf.get_variable_scope().reuse)\n",
    "        elif self.cell_type == self.RNN_NAS_CELL:\n",
    "            single_cell = tf.contrib.rnn.LayerNormBasicLSTMCell(\n",
    "                self.num_units)\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown rnn cell type. {self.cell_type}\")\n",
    "\n",
    "        if self.dropout > 0.0:\n",
    "            single_cell = tf.contrib.rnn.DropoutWrapper(\n",
    "                cell=single_cell, input_keep_prob=(1.0 - self.dropout))\n",
    "        return single_cell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tensorflow as tf\n",
    "# __all__ = [\"Attention\", \"Decoder\"]\n",
    "\n",
    "\n",
    "class Attention:\n",
    "    \"\"\"Attention class \"\"\"\n",
    "    \n",
    "    BAHDANAU_MECHANISM = \"bahdanau\"\n",
    "    NORMED_BAHDANAU_MECHANISM = \"normed_bahdanau\"\n",
    "\n",
    "    LUONG_MECHANISM = \"luong\"\n",
    "    SCALED_LUONG_MECHANISM = \"scaled_luong\"\n",
    "    \n",
    "    def __init__(self,\n",
    "                 attention_mechanism=\"bahdanau\",\n",
    "                 encoder_type=\"bi\",\n",
    "                 num_units=512,\n",
    "                 memory=None,\n",
    "                 memory_sequence_length=None):\n",
    "        \n",
    "        assert memory is not None\n",
    "        assert memory_sequence_length is not None\n",
    "        \n",
    "        self.attention_mechanism = attention_mechanism\n",
    "        self.encoder_type = encoder_type\n",
    "        self.num_units = num_units\n",
    "        self.memory = memory\n",
    "        self.memory_sequence_length = memory_sequence_length\n",
    "        \n",
    "    \n",
    "    def wrap(self, decoder_cell, alignment_history=True):\n",
    "        with tf.variable_scope(\"attention\") as scope:\n",
    "            attention_layer_size = self.num_units\n",
    "            return tf.contrib.seq2seq.AttentionWrapper(\n",
    "                        decoder_cell,\n",
    "                        self._create_mechanism(),\n",
    "                        attention_layer_size=attention_layer_size,\n",
    "                        alignment_history=alignment_history,\n",
    "                        name=f\"{self.attention_mechanism}-mechanism\")\n",
    "        \n",
    "        \n",
    "    \n",
    "    def _create_mechanism(self):\n",
    "\n",
    "        if self.attention_mechanism == \"bahdanau\":\n",
    "            return tf.contrib.seq2seq.BahdanauAttention(\n",
    "                    self.num_units,\n",
    "                    self.memory,\n",
    "                    memory_sequence_length=self.memory_sequence_length)\n",
    "\n",
    "        elif self.attention_mechanism == \"normed_bahdanau\":\n",
    "            return tf.contrib.seq2seq.BahdanauAttention(\n",
    "                    self.num_units,\n",
    "                    self.memory,\n",
    "                    memory_sequence_length=self.memory_sequence_length,\n",
    "                    normalize=True)\n",
    "\n",
    "        elif self.attention_mechanism == \"luong\":\n",
    "            return tf.contrib.seq2seq.LuongAttention(\n",
    "                    self.num_units,\n",
    "                    self.memory,\n",
    "                    memory_sequence_length=self.memory_sequence_length)\n",
    "\n",
    "        elif self.attention_mechanism == \"scaled_luong\":\n",
    "            return tf.contrib.seq2seq.LuongAttention(\n",
    "                    self.num_units,\n",
    "                    self.memory,\n",
    "                    memory_sequence_length=self.memory_sequence_length,\n",
    "                    scale=True)\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown attention mechanism {self.attention_mechanism}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decoder Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder:\n",
    "    \"\"\"Decoder class\"\"\"\n",
    "\n",
    "    UNI_ENCODER_TYPE = \"uni\"\n",
    "    BI_ENCODER_TYPE = \"bi\"\n",
    "\n",
    "    RNN_GRU_CELL = \"gru\"\n",
    "    RNN_LSTM_CELL = \"lstm\"\n",
    "    RNN_LAYER_NORM_LSTM_CELL = \"layer_norm_lstm\"\n",
    "    RNN_NAS_CELL = \"nas\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 cell_type=\"lstm\",\n",
    "                 dropout=0.8,\n",
    "                 encoder_type=\"uni\",\n",
    "                 num_layers=None,\n",
    "                 num_units=None,\n",
    "                 sampling_probability=0.4,\n",
    "                 mode=tf.estimator.ModeKeys.TRAIN,\n",
    "                 dtype=tf.float32):\n",
    "\n",
    "        self.cell_type = cell_type\n",
    "        self.dropout = dropout\n",
    "        self.encoder_type = encoder_type\n",
    "        self.num_layers = num_layers\n",
    "        self.num_units = num_units\n",
    "        self.sampling_probability = sampling_probability\n",
    "\n",
    "        if encoder_type == self.BI_ENCODER_TYPE:\n",
    "            self.num_units *= 2\n",
    "            self.num_layers = int(self.num_layers / 2)\n",
    "            if self.num_layers == 0:\n",
    "                self.num_layers = 1\n",
    "        self.mode = mode\n",
    "        self.dtype = dtype\n",
    "\n",
    "        \n",
    "    def set_attention_then_project(self,\n",
    "                                   attention_mechanism=\"bahdanau\",\n",
    "                                   beam_width=0,\n",
    "                                   memory=None,\n",
    "                                   memory_sequence_length=None,\n",
    "                                   vocab_size=None):\n",
    "\n",
    "        self.beam_width = beam_width\n",
    "\n",
    "        cells = self._create_rnn_cells()\n",
    "\n",
    "        attention = Attention(\n",
    "                        attention_mechanism=attention_mechanism,\n",
    "                        encoder_type=self.encoder_type,\n",
    "                        num_units=self.num_units,\n",
    "                        memory=memory,\n",
    "                        memory_sequence_length=memory_sequence_length)\n",
    "        alignment_history = (self.mode == tf.estimator.ModeKeys.PREDICT\n",
    "                and self.beam_width == 0)\n",
    "\n",
    "        attn_cell = attention.wrap(cells, alignment_history=alignment_history)\n",
    "        self.out_cell = tf.contrib.rnn.OutputProjectionWrapper(\n",
    "            attn_cell, vocab_size)\n",
    "\n",
    "        self.maximum_iterations = tf.round(tf.reduce_max(memory_sequence_length) * 2)\n",
    "\n",
    "    def set_initial_state(self, batch_size, encoder_final_state):\n",
    "        if self.mode == tf.estimator.ModeKeys.PREDICT and self.beam_width > 0:\n",
    "            decoder_start_state = tf.contrib.seq2seq.tile_batch(encoder_final_state, self.beam_width)\n",
    "            self.decoder_initial_state = self.out_cell.zero_state(batch_size * self.beam_width, self.dtype)\n",
    "            self.decoder_initial_state = self.decoder_initial_state.clone(cell_state=decoder_start_state)\n",
    "        else:\n",
    "            self.decoder_initial_state = self.out_cell.zero_state(batch_size, self.dtype)\n",
    "            self.decoder_initial_state = self.decoder_initial_state.clone(cell_state=encoder_final_state)\n",
    "\n",
    "            \n",
    "            \n",
    "    def build(self,\n",
    "            inputs=None,\n",
    "            sequence_length=None,\n",
    "            embedding=None,\n",
    "            start_tokens=None,\n",
    "            end_token=None,\n",
    "            length_penalty_weight=1.0):\n",
    "\n",
    "        if self.mode == tf.estimator.ModeKeys.TRAIN:\n",
    "            assert inputs is not None\n",
    "            assert sequence_length is not None\n",
    "\n",
    "            helper = tf.contrib.seq2seq.ScheduledEmbeddingTrainingHelper(\n",
    "                    inputs=inputs,\n",
    "                    sequence_length=sequence_length,\n",
    "                    embedding=embedding,\n",
    "                    sampling_probability=self.sampling_probability)\n",
    "\n",
    "            return self._basic_decoder(helper)\n",
    "\n",
    "        else:\n",
    "            assert embedding is not None\n",
    "            assert start_tokens is not None\n",
    "            assert end_token is not None\n",
    "\n",
    "            if self.mode == tf.estimator.ModeKeys.PREDICT and self.beam_width > 0:\n",
    "                return self._beam_search_decoder(\n",
    "                        embedding, start_tokens, end_token, length_penalty_weight)\n",
    "            else:\n",
    "                helper = tf.contrib.seq2seq.GreedyEmbeddingHelper(\n",
    "                        embedding=embedding,\n",
    "                        start_tokens=start_tokens,\n",
    "                        end_token=end_token)\n",
    "                return self._basic_decoder(helper)\n",
    "            \n",
    "\n",
    "    def _basic_decoder(self, helper):\n",
    "        decoder = tf.contrib.seq2seq.BasicDecoder(\n",
    "            cell=self.out_cell,\n",
    "            helper=helper,\n",
    "            initial_state=self.decoder_initial_state)\n",
    "\n",
    "        if self.mode == tf.estimator.ModeKeys.TRAIN:\n",
    "            outputs, final_context_state, _ = tf.contrib.seq2seq.dynamic_decode(\n",
    "                decoder=decoder,\n",
    "                output_time_major=False,\n",
    "                impute_finished=True,\n",
    "                swap_memory=True)\n",
    "        else:\n",
    "            outputs, final_context_state, _ = tf.contrib.seq2seq.dynamic_decode(\n",
    "                decoder=decoder,\n",
    "                output_time_major=False,\n",
    "                impute_finished=True,\n",
    "                maximum_iterations=self.maximum_iterations)\n",
    "\n",
    "        return outputs\n",
    "\n",
    "    \n",
    "    \n",
    "    def _beam_search_decoder(self, embedding, start_tokens, end_token, length_penalty_weight):\n",
    "        decoder = tf.contrib.seq2seq.BeamSearchDecoder(\n",
    "            cell=self.out_cell,\n",
    "            embedding=embedding,\n",
    "            start_tokens=start_tokens,\n",
    "            end_token=end_token,\n",
    "            initial_state=self.decoder_initial_state,\n",
    "            beam_width=self.beam_width,\n",
    "            length_penalty_weight=length_penalty_weight)\n",
    "\n",
    "        outputs, final_context_state, _ = tf.contrib.seq2seq.dynamic_decode(\n",
    "            decoder=decoder,\n",
    "            output_time_major=False,\n",
    "            impute_finished=False,\n",
    "            maximum_iterations=self.maximum_iterations)\n",
    "        return outputs\n",
    "    \n",
    "    \n",
    "\n",
    "    def _create_rnn_cells(self):\n",
    "        \"\"\"Contructs stacked_rnn with num_layers\n",
    "        * Args:\n",
    "            is_list: flags for stack bidirectional. True=stack bidirectional, False=unidirectional\n",
    "        * Returns:\n",
    "            stacked_rnn\n",
    "        \"\"\"\n",
    "\n",
    "        stacked_rnn = []\n",
    "        for _ in range(self.num_layers):\n",
    "            single_cell = self._rnn_single_cell()\n",
    "            stacked_rnn.append(single_cell)\n",
    "\n",
    "        if self.num_layers == 1:\n",
    "            return stacked_rnn[0]\n",
    "        else:\n",
    "            return tf.nn.rnn_cell.MultiRNNCell(\n",
    "                    cells=stacked_rnn,\n",
    "                    state_is_tuple=True)\n",
    "        \n",
    "        \n",
    "\n",
    "    def _rnn_single_cell(self):\n",
    "        \"\"\"Contructs rnn single_cell\"\"\"\n",
    "\n",
    "        if self.cell_type == self.RNN_GRU_CELL:\n",
    "            single_cell = tf.contrib.rnn.GRUCell(\n",
    "                self.num_units,\n",
    "                reuse=tf.get_variable_scope().reuse)\n",
    "        elif self.cell_type == self.RNN_LSTM_CELL:\n",
    "            single_cell = tf.contrib.rnn.BasicLSTMCell(\n",
    "                self.num_units,\n",
    "                forget_bias=1.0,\n",
    "                reuse=tf.get_variable_scope().reuse)\n",
    "        elif self.cell_type == self.RNN_LAYER_NORM_LSTM_CELL:\n",
    "            single_cell = tf.contrib.rnn.LayerNormBasicLSTMCell(\n",
    "                self.num_units,\n",
    "                forget_bias=1.0,\n",
    "                layer_norm=True,\n",
    "                reuse=tf.get_variable_scope().reuse)\n",
    "        elif self.cell_type == self.RNN_NAS_CELL:\n",
    "            single_cell = tf.contrib.rnn.LayerNormBasicLSTMCell(\n",
    "                self.num_units)\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown rnn cell type. {self.cell_type}\")\n",
    "\n",
    "        if self.dropout > 0.0:\n",
    "            single_cell = tf.contrib.rnn.DropoutWrapper(\n",
    "                cell=single_cell, input_keep_prob=(1.0 - self.dropout))\n",
    "\n",
    "        return single_cell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graph : combine Encoder & Decoder Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Graph:\n",
    "\n",
    "    def __init__(self, mode=None, dtype=tf.float32):\n",
    "        self.mode = mode\n",
    "        self.beam_width = Config.predict.get('beam_width', 0)\n",
    "        self.dtype = dtype\n",
    "\n",
    "    def build(self,\n",
    "              encoder_inputs=None,\n",
    "              decoder_inputs=None):\n",
    "\n",
    "        # set inputs variable\n",
    "        self.encoder_inputs = encoder_inputs\n",
    "        self.encoder_input_lengths = tf.reduce_sum(\n",
    "                     tf.to_int32(tf.not_equal(self.encoder_inputs, Config.data.PAD_ID)), 1,\n",
    "                     name=\"encoder_input_lengths\")\n",
    "\n",
    "        if self.mode == tf.estimator.ModeKeys.TRAIN or self.mode == tf.estimator.ModeKeys.EVAL:\n",
    "            self.decoder_inputs = decoder_inputs\n",
    "            self.decoder_input_lengths = tf.reduce_sum(\n",
    "                tf.to_int32(tf.not_equal(self.decoder_inputs, Config.data.PAD_ID)), 1,\n",
    "                name=\"decoder_input_lengths\")\n",
    "        else:\n",
    "            self.decoder_inputs = None\n",
    "            self.decoder_input_lengths = None\n",
    "\n",
    "        # self._build_embed()  # 不需要 Embed\n",
    "        self._build_encoder()\n",
    "        self._build_decoder()\n",
    "\n",
    "\n",
    "    def _build_embed(self):\n",
    "        with tf.variable_scope (\"embeddings\", dtype=self.dtype) as scope:\n",
    "            self.embedding_decoder = tf.get_variable(\n",
    "                       \"embedding_decoder\", [Config.data.vocab_size, Config.model.embed_dim], self.dtype)\n",
    "            if self.mode == tf.estimator.ModeKeys.TRAIN:\n",
    "                self.decoder_emb_inp = tf.nn.embedding_lookup(\n",
    "                    self.embedding_decoder, self.decoder_inputs)\n",
    "            else:\n",
    "                self.decoder_emb_inp=None\n",
    "            \n",
    "        \n",
    "'''\n",
    "        with tf.variable_scope (\"embeddings\", dtype=self.dtype) as scope:\n",
    "            if Config.model.embed_share:\n",
    "                embedding = tf.get_variable(\n",
    "                    \"embedding_share\", [Config.data.vocab_size, Config.model.embed_dim], self.dtype)\n",
    "\n",
    "                self.embedding_encoder = embedding\n",
    "                self.embedding_decoder = embedding\n",
    "            else:\n",
    "                self.embedding_encoder = tf.get_variable(\n",
    "                    \"embedding_encoder\", [Config.data.vocab_size, Config.model.embed_dim], self.dtype)\n",
    "            self.embedding_decoder = tf.get_variable(\n",
    "                    \"embedding_decoder\", [Config.data.vocab_size, Config.model.embed_dim], self.dtype)\n",
    "\n",
    "            self.encoder_emb_inp = tf.nn.embedding_lookup(\n",
    "                self.embedding_encoder, self.encoder_inputs)\n",
    "\n",
    "            if self.mode == tf.estimator.ModeKeys.TRAIN:\n",
    "                self.decoder_emb_inp = tf.nn.embedding_lookup(\n",
    "                    self.embedding_decoder, self.decoder_inputs)\n",
    "            else:\n",
    "                self.decoder_emb_inp=None\n",
    "'''\n",
    "\n",
    "\n",
    "\n",
    "    def _build_encoder(self):\n",
    "        with tf.variable_scope('encoder'):\n",
    "            encoder = Encoder(\n",
    "                        encoder_type = Config.model.encoder_type,\n",
    "                        num_layers = Config.model.num_layers,\n",
    "                        cell_type = Config.model.cell_type,\n",
    "                        num_units = Config.model.num_units,\n",
    "                        dropout = Config.model.dropout)\n",
    "\n",
    "            self.encoder_outputs, self.encoder_final_state = encoder.build(\n",
    "                                  input_vector = self.encoder_inputs #self.encoder_emb_inp,\n",
    "                                  sequence_length = self.encoder_input_lengths)\n",
    "\n",
    "            if self.mode == tf.estimator.ModeKeys.PREDICT and self.beam_width > 0:\n",
    "                self.encoder_outputs = tf.contrib.seq2seq.tile_batch(\n",
    "                                          self.encoder_outputs, self.beam_width )\n",
    "                self.encoder_input_lengths = tf.contrib.seq2seq.tile_batch(\n",
    "                                          self.encoder_input_lengths, self.beam_width )\n",
    "\n",
    "    def _build_decoder(self):\n",
    "\n",
    "        batch_size = tf.shape(self.encoder_inputs)[0]\n",
    "\n",
    "        with tf.variable_scope('decoder'):\n",
    "\n",
    "            decoder = Decoder(\n",
    "                        cell_type = Config.model.cell_type,\n",
    "                        dropout = Config.model.dropout,\n",
    "                        encoder_type = Config.model.encoder_type,\n",
    "                        num_layers = Config.model.num_layers,\n",
    "                        num_units = Config.model.num_units,\n",
    "                        sampling_probability = Config.train.sampling_probability,\n",
    "                        mode = self.mode,\n",
    "                        dtype = self.dtype)\n",
    "\n",
    "            decoder.set_attention_then_project(\n",
    "                        attention_mechanism=Config.model.attention_mechanism,\n",
    "                        beam_width=self.beam_width,\n",
    "                        memory=self.encoder_outputs,\n",
    "                        memory_sequence_length=self.encoder_input_lengths,\n",
    "                        vocab_size=Config.data.vocab_size)\n",
    "            decoder.set_initial_state(batch_size, self.encoder_final_state)\n",
    "\n",
    "            decoder_outputs = decoder.build(\n",
    "                                inputs=self.decoder_inputs #self.decoder_emb_inp,\n",
    "                                sequence_length=self.decoder_input_lengths,\n",
    "                                embedding=self.embedding_decoder,\n",
    "                                start_tokens=tf.fill([batch_size], Config.data.START_ID),\n",
    "                                end_token=Config.data.EOS_ID,\n",
    "                                length_penalty_weight=Config.predict.length_penalty_weight)\n",
    "\n",
    "            if self.mode == tf.estimator.ModeKeys.TRAIN:\n",
    "                self.decoder_logits = decoder_outputs.rnn_output\n",
    "            else:\n",
    "                if self.mode == tf.estimator.ModeKeys.PREDICT and self.beam_width > 0:\n",
    "                    self.decoder_logits = tf.no_op()\n",
    "                    self.predictions = decoder_outputs.predicted_ids\n",
    "                else:\n",
    "                    self.decoder_logits = decoder_outputs.rnn_output\n",
    "                    self.predictions = decoder_outputs.sample_id\n",
    "\n",
    "            if self.mode == tf.estimator.ModeKeys.PREDICT:\n",
    "                # PREDICT mode do not need loss\n",
    "                return\n",
    "\n",
    "            decoder_output_length = tf.shape(self.decoder_logits)[1]\n",
    "\n",
    "            def concat_zero_padding():\n",
    "                pad_num = Config.data.max_seq_length - decoder_output_length\n",
    "                zero_padding = tf.zeros(\n",
    "                        [batch_size, pad_num, Config.data.vocab_size],\n",
    "                        dtype=self.dtype)\n",
    "\n",
    "                return tf.concat([self.decoder_logits, zero_padding], axis=1)\n",
    "\n",
    "            def slice_to_max_len():\n",
    "                return tf.slice(self.decoder_logits,\n",
    "                                [0, 0, 0],\n",
    "                                [batch_size, Config.data.max_seq_length, Config.data.vocab_size])\n",
    "\n",
    "            # decoder output sometimes exceed max_seq_length\n",
    "            self.logits = tf.cond(decoder_output_length < Config.data.max_seq_length,\n",
    "                                  concat_zero_padding,\n",
    "                                  slice_to_max_len)\n",
    "            self.predictions = tf.argmax(self.logits, axis=2)\n",
    "\n",
    "            self.weight_masks = tf.sequence_mask(\n",
    "                lengths=self.decoder_input_lengths,\n",
    "                maxlen=Config.data.max_seq_length,\n",
    "                dtype=self.dtype, name='masks')\n",
    "\n",
    "        if self.mode == tf.estimator.ModeKeys.TRAIN:\n",
    "            self.train_predictions = tf.argmax(self.logits, axis=2)\n",
    "            # for print trainig data\n",
    "            tf.identity(tf.argmax(self.decoder_logits[0], axis=1), name='train/pred_0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# seq2seq Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tensorflow as tf\n",
    "\n",
    "class seq2seq:\n",
    "    def __init__():\n",
    "        pass\n",
    "    \n",
    "    def model_fn(self, mode, features, labels, params):\n",
    "        self.dtype = tf.float32\n",
    "        self.mode = mode\n",
    "        \n",
    "        self.loss, self.train_op, self.metrics, self.predictions = None, None, None, None\n",
    "        # initializing train data\n",
    "        self._init_placeholder(features, labels)\n",
    "        #\n",
    "        self.build_graph()\n",
    "\n",
    "        # train mode: required loss and train_op\n",
    "        # eval mode: required loss\n",
    "        # predict mode: required predictions\n",
    "\n",
    "        return tf.estimator.EstimatorSpec(\n",
    "                                    mode = mode,\n",
    "                                    loss = self.loss,\n",
    "                                    train_op = self.train_op,\n",
    "                                    eval_metric_ops = self.metrics,\n",
    "                                    predictions = {\"prediction\": self.predictions})\n",
    "  \n",
    "\n",
    "\n",
    "    def _init_placeholder(self, features, labels):\n",
    "        \n",
    "        ''' Declare Encoder input data'''\n",
    "        self.encoder_inputs = features\n",
    "        if type(features) == dict:\n",
    "            self.encoder_inputs = features[\"input_data\"]\n",
    "  \n",
    "        ''' '''\n",
    "        batch_size = tf.shape(self.encoder_inputs)[0]\n",
    "        \n",
    "        \n",
    "        if self.mode == tf.estimator.ModeKeys.TRAIN or self.mode == tf.estimator.ModeKeys.EVAL:\n",
    "            ''' Declare Decoder label data'''\n",
    "            self.decoder_inputs = labels\n",
    "            decoder_input_shift_1 = tf.slice(self.decoder_inputs, \n",
    "                                             [0, 1], #begin\n",
    "                                             [batch_size, Config.data.max_seq_length-1]) # size\n",
    "            pad_tokens = tf.zeros([batch_size, 1], dtype=tf.int32)\n",
    "\n",
    "            # make target (right shift 1 from decoder_inputs)\n",
    "            self.targets = tf.concat([decoder_input_shift_1, pad_tokens], axis=1)\n",
    "        else:\n",
    "            self.decoder_inputs = None\n",
    "            \n",
    "\n",
    "            \n",
    "\n",
    "    def build_graph(self):\n",
    "        graph = seq2seq_attention.Graph(mode=self.mode) # call encoder & decoder model\n",
    "        graph.build(encoder_inputs=self.encoder_inputs,\n",
    "                    decoder_inputs=self.decoder_inputs)\n",
    "\n",
    "        self.predictions = graph.predictions\n",
    "        if self.mode != tf.estimator.ModeKeys.PREDICT:\n",
    "            self._build_loss(graph.logits, graph.weight_masks)\n",
    "            self._build_optimizer()\n",
    "            self._build_metric()\n",
    "            \n",
    "            \n",
    "\n",
    "    def _build_loss(self, logits, weight_masks):\n",
    "        self.loss = tf.contrib.seq2seq.sequence_loss(\n",
    "                logits=logits,\n",
    "                targets=self.targets,\n",
    "                weights=weight_masks,\n",
    "                name=\"loss\")\n",
    "        \n",
    "        \n",
    "\n",
    "    def _build_optimizer(self):\n",
    "        self.train_op = tf.contrib.layers.optimize_loss(\n",
    "            self.loss, tf.train.get_global_step(),\n",
    "            optimizer='Adam',\n",
    "            learning_rate=Config.train.learning_rate,\n",
    "            summaries=['loss', 'learning_rate'],\n",
    "            name=\"train_op\")\n",
    "        \n",
    "        \n",
    "\n",
    "    def _build_metric(self):\n",
    "\n",
    "        def blue_score(labels, predictions,\n",
    "                       weights=None, metrics_collections=None,\n",
    "                       updates_collections=None, name=None):\n",
    "\n",
    "            def _nltk_blue_score(labels, predictions):\n",
    "\n",
    "                # slice after <eos>\n",
    "                predictions = predictions.tolist()\n",
    "                for i in range(len(predictions)):\n",
    "                    prediction = predictions[i]\n",
    "                    if Config.data.EOS_ID in prediction:\n",
    "                        predictions[i] = prediction[:prediction.index(Config.data.EOS_ID)+1]\n",
    "\n",
    "                labels = [\n",
    "                    [[w_id for w_id in label if w_id != Config.data.PAD_ID]]\n",
    "                    for label in labels.tolist()]\n",
    "                predictions = [\n",
    "                    [w_id for w_id in prediction]\n",
    "                    for prediction in predictions]\n",
    "\n",
    "                return float(nltk.translate.bleu_score.corpus_bleu(labels, predictions))\n",
    "\n",
    "            score = tf.py_func(_nltk_blue_score, (labels, predictions), tf.float64)\n",
    "            return tf.metrics.mean(score * 100)\n",
    "\n",
    "        self.metrics = {\n",
    "            \"accuracy\": tf.metrics.accuracy(self.targets, self.predictions),\n",
    "            \"bleu\": blue_score(self.targets, self.predictions)\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import logging\n",
    "\n",
    "from hbconfig import Config\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def experiment_fn(run_config, params):\n",
    "\n",
    "    conversation = seq2seq() # Declare S2S model\n",
    "    \n",
    "    \n",
    "    estimator = tf.estimator.Estimator(\n",
    "                   model_fn=seq2seq.model_fn,\n",
    "                   model_dir=Config.train.model_dir, # \n",
    "                   params=params,\n",
    "                   config=run_config)\n",
    "    \n",
    "    \n",
    "    #Loading data\n",
    "    vocab = data_loader.load_data('data.csv')\n",
    "    \n",
    "    \n",
    "    Config.data.vocab_size = len(vocab)\n",
    "    \n",
    "    # spliting train & test data\n",
    "    train_X, test_X, train_y, test_y = data_loader.make_train_and_test_set()\n",
    "\n",
    "    # batch_size\n",
    "    train_input_fn, train_input_hook = data_loader.make_batch((train_X, train_y), batch_size=Config.model.batch_size)\n",
    "    test_input_fn, test_input_hook = data_loader.make_batch((test_X, test_y), batch_size=Config.model.batch_size, scope=\"test\")\n",
    "\n",
    "    \n",
    "    train_hooks = [train_input_hook]\n",
    "    if Config.train.print_verbose:\n",
    "        train_hooks.append(hook.print_variables(\n",
    "            variables=['train/enc_0', 'train/dec_0', 'train/pred_0'],\n",
    "            rev_vocab=utils.get_rev_vocab(vocab),\n",
    "            every_n_iter=Config.train.check_hook_n_iter))\n",
    "    if Config.train.debug:\n",
    "        train_hooks.append(tf_debug.LocalCLIDebugHook())\n",
    "\n",
    "    eval_hooks = [test_input_hook]\n",
    "    if Config.train.debug:\n",
    "        eval_hooks.append(tf_debug.LocalCLIDebugHook())\n",
    "\n",
    "    experiment = tf.contrib.learn.Experiment(\n",
    "        estimator=estimator,\n",
    "        train_input_fn=train_input_fn,\n",
    "        eval_input_fn=test_input_fn,\n",
    "        train_steps=Config.train.train_steps,\n",
    "        min_eval_frequency=Config.train.min_eval_frequency,\n",
    "        train_monitors=train_hooks,\n",
    "        eval_hooks=eval_hooks,\n",
    "        eval_delay_secs=0\n",
    "    )\n",
    "    return experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nif __name__ == '__main__':\\n    main('train')\\n\""
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def main(mode):\n",
    "    params = tf.contrib.training.HParams(**Config.model.to_dict())\n",
    "\n",
    "    run_config = tf.contrib.learn.RunConfig(\n",
    "                      model_dir=Config.train.model_dir,\n",
    "                      save_checkpoints_steps=Config.train.save_checkpoints_steps)\n",
    "\n",
    "    tf.contrib.learn.learn_runner.run(\n",
    "                       experiment_fn=experiment_fn,\n",
    "                       run_config=run_config,\n",
    "                       schedule=mode,\n",
    "                       hparams=params\n",
    "                                     )\n",
    "\n",
    "\n",
    "'''\n",
    "if __name__ == '__main__':\n",
    "    main('train')\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import os\n",
    "import re\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\"\"\"\n",
    "from hbconfig import Config\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tqdm import tqdm\n",
    "\"\"\" \n",
    "\n",
    "class data_loader:\n",
    "\n",
    "    def make_dir(self, path):\n",
    "        \"\"\" Create a directory if there isn't one.\"\"\"\n",
    "        try:\n",
    "            os.mkdir(path)\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "\n",
    "\n",
    "    def load_data(self, file_name):\n",
    "        print(\"load data ...\")\n",
    "        data = pd.read_csv(file_name) # ｘ,ｙ　\n",
    "        \n",
    "\n",
    "        \n",
    "        \n",
    "    def make_batch(data, buffer_size=10000, batch_size=64, scope=\"train\"):\n",
    "        \n",
    "        class IteratorInitializerHook(tf.train.SessionRunHook):\n",
    "            \"\"\"Hook to initialise data iterator after Session is created.\"\"\"\n",
    "            def __init__(self):\n",
    "                super(IteratorInitializerHook, self).__init__()\n",
    "                self.iterator_initializer_func = None\n",
    "            def after_create_session(self, session, coord):\n",
    "                \"\"\"Initialise the iterator after the session has been created.\"\"\"\n",
    "                self.iterator_initializer_func(session)\n",
    "        \n",
    "        def get_inputs():\n",
    "            iterator_initializer_hook = IteratorInitializerHook()\n",
    "\n",
    "            def train_inputs():\n",
    "                with tf.name_scope(scope): # scope = \"train\"\n",
    "                    X, y = data # <<<<====\n",
    "\n",
    "                    # Define placeholders\n",
    "                    input_placeholder = tf.placeholder(\n",
    "                                             tf.int32, [None, Config.data.max_seq_length])\n",
    "                    output_placeholder = tf.placeholder(\n",
    "                                             tf.int32, [None, Config.data.max_seq_length])\n",
    "\n",
    "                    # Build dataset iterator\n",
    "                    dataset = tf.data.Dataset.from_tensor_slices(\n",
    "                                              (input_placeholder, output_placeholder)\n",
    "                                                                )\n",
    "\n",
    "                    if scope == \"train\":\n",
    "                        dataset = dataset.repeat(None)  # Infinite iterations\n",
    "                    else:\n",
    "                        dataset = dataset.repeat(1)  # 1 Epoch\n",
    "                        \n",
    "                    # dataset = dataset.shuffle(buffer_size=buffer_size)\n",
    "                    dataset = dataset.batch(batch_size)\n",
    "\n",
    "                    iterator = dataset.make_initializable_iterator()\n",
    "                    next_X, next_y = iterator.get_next()\n",
    "\n",
    "                    tf.identity(next_X[0], 'enc_0')\n",
    "                    tf.identity(next_y[0], 'dec_0')\n",
    "\n",
    "                    # Set runhook to initialize iterator\n",
    "                    iterator_initializer_hook.iterator_initializer_func = \\\n",
    "                        lambda sess: sess.run(\n",
    "                            iterator.initializer,\n",
    "                            feed_dict={input_placeholder: X,\n",
    "                                       output_placeholder: y})\n",
    "\n",
    "                    # Return batched (features, labels)\n",
    "                    return next_X, next_y\n",
    "\n",
    "            # Return function and hook\n",
    "            return train_inputs, iterator_initializer_hook\n",
    "\n",
    "        return get_inputs()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
