{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.layers import core as layers_core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow Version 1.6.0\n"
     ]
    }
   ],
   "source": [
    "print(\"TensorFlow Version\", tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input shape:(52999, 41) , target shape:(52999, 71) \n"
     ]
    }
   ],
   "source": [
    "# Generating data\n",
    "\n",
    "def generating_data(input_len=41, output_len=71):\n",
    "    x = np.random.randint(2, size=(52999, 41))\n",
    "    y = np.random.randint(2, size=(52999, 71))\n",
    "    return x, y\n",
    "x_input, y_label = generating_data()\n",
    "print(\"input shape:{} , target shape:{} \".format(x_input.shape, y_label.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Batch Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_generator(x_input,\n",
    "                    y_label,\n",
    "                    batch_size=5, \n",
    "                    seq_len=10,\n",
    "                    test_ratio=0.1):\n",
    "    \n",
    "    assert y_label.shape[0]==x_input.shape[0], \"no. of x & y are different \" # Check data size\n",
    "    \n",
    "    ''' Generating eaach batch data(np.array) into a list'''\n",
    "    total_batchs = int(y_label.shape[0] / seq_len / batch_size) # \n",
    "    test_batchs = int(total_batchs * test_ratio)\n",
    "    train_batchs = total_batchs - test_batchs\n",
    "    \n",
    "\n",
    "    print('unuse data', (train_batchs + test_batchs)*batch_size*seq_len - y_label.shape[0])\n",
    "    print('Train Batch: {} ; Test Batch: {}'.format(train_batchs, test_batchs))\n",
    "    \n",
    "    train_input = []\n",
    "    train_target = []\n",
    "    test_input = []\n",
    "    test_target = []\n",
    "\n",
    "    up = 0\n",
    "    down = seq_len\n",
    "    \n",
    "    for _ in range(train_batchs):\n",
    "        # data size in one batch [batch size, seq length, vector length]\n",
    "        x = np.zeros(shape=(batch_size, seq_len, x_input.shape[1]))\n",
    "        y = np.zeros(shape=(batch_size, seq_len, y_label.shape[1]))\n",
    "        for b in range(batch_size):\n",
    "            x[b, :, :] = x_input[up : down, :]\n",
    "            y[b, :, :] = y_label[up : down, :]\n",
    "            up = down\n",
    "            down = down+seq_len\n",
    "            #print(up, ':', down)\n",
    "        train_input.append(x)\n",
    "        train_target.append(y)\n",
    "        \n",
    "    for _ in range(test_batchs):\n",
    "        x = np.zeros(shape=(batch_size, seq_len, x_input.shape[1]))\n",
    "        y = np.zeros(shape=(batch_size, seq_len, y_label.shape[1]))\n",
    "        for b in range(batch_size):\n",
    "            #print(b, x.shape, range(up,down))\n",
    "            x[b, :, :] = x_input[up : down, :]\n",
    "            y[b, :, :] = y_label[up : down, :]\n",
    "            up = down\n",
    "            down = down+seq_len\n",
    "            #print('Test data',up, ':', down)\n",
    "        test_input.append(x)\n",
    "        test_target.append(y)\n",
    "    return train_input, train_target, test_input, test_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Seq2Seq with Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2Seq:\n",
    "    def __init__(self,\n",
    "                 seq_max_len=1.,  \n",
    "                 input_len=41.,\n",
    "                 output_len=71.,\n",
    "                 batch_size=1,\n",
    "                 lstm_size=[128., 128., 128.],\n",
    "                 learning_rate=0.005,\n",
    "                 grad_clip=2.,\n",
    "                 keep_prob=0.8,\n",
    "                 forward_only= None):\n",
    "        \n",
    "        self.seq_max_len = seq_max_len\n",
    "        self.batch_size = batch_size\n",
    "        self.batch_len = np.array([])\n",
    "        self.input_len = input_len\n",
    "        self.output_len = output_len\n",
    "        self.lstm_size = lstm_size\n",
    "        self.num_units = self.lstm_size[-1]\n",
    "        self.num_layers = len(self.lstm_size)\n",
    "        self.learning_rate = learning_rate\n",
    "        self.grad_clip = grad_clip\n",
    "        self.train_keep_prob = keep_prob\n",
    "        self.go_token = 9.\n",
    "        self.batch_seq_len = np.int32(np.ones(shape=([self.batch_size])) * self.seq_max_len)\n",
    "        \n",
    "        \n",
    "        \n",
    "        '''Executing function'''\n",
    "        tf.reset_default_graph() #Clears the default graph stack and resets the global default graph\n",
    "        self.build_inputs()\n",
    "        self.build_encoder()\n",
    "        self.build_decoder()\n",
    "        self.build_loss()\n",
    "        self.build_optimizer()\n",
    "        self.saver = tf.train.Saver() #Saves and restores variables.\n",
    " \n",
    "\n",
    "    def build_inputs(self):\n",
    "        self.encoder_inputs = tf.placeholder(tf.float32, \n",
    "                                         shape=(self.batch_size, self.seq_max_len, self.input_len),\n",
    "                                         name='inputs')\n",
    "        self.targets = tf.placeholder(tf.float32,\n",
    "                                         shape=(self.batch_size, self.seq_max_len, self.output_len),\n",
    "                                         name='targets')\n",
    "        self.decoder_inputs = tf.placeholder(tf.float32,\n",
    "                                                shape=(self.batch_size, self.seq_max_len, self.output_len),  \n",
    "                                                name='decoder_inputs')        \n",
    "        self.keep_prob = tf.placeholder(tf.float32, name='keep_prob')\n",
    "         \n",
    "        '''if seq is different， sequence input was needed :\n",
    "        self.input_sequence_length = tf.placeholder(shape=([self.batch_size]), dtype=tf.int32, name='input_length')\n",
    "        self.decoder_sequence_length = tf.placeholder(shape=([self.batch_size]), dtype=tf.int32, name='decoder_inputs_length')\n",
    "        self.target_sequence_length = tf.placeholder(shape=([self.batch_size]), dtype=tf.float32, name='target_sequence_length')\n",
    "        '''\n",
    "        \n",
    "\n",
    "    # Encoder Model==========================================================================\n",
    "    def build_encoder(self):\n",
    "        ''' Encoder Model'''\n",
    "        def get_a_cell(lstm_size, keep_prop):\n",
    "            lstm = tf.nn.rnn_cell.BasicLSTMCell(lstm_size)\n",
    "            drop = tf.nn.rnn_cell.DropoutWrapper(lstm, output_keep_prob=self.train_keep_prob)\n",
    "            return drop\n",
    "\n",
    "        with tf.variable_scope('encoder', initializer=tf.orthogonal_initializer()):\n",
    "            encoder_cell = tf.nn.rnn_cell.MultiRNNCell(\n",
    "                                 [get_a_cell(size, self.keep_prob) for size in self.lstm_size]\n",
    "                                                      )\n",
    "            self.initial_state = encoder_cell.zero_state(self.batch_size, tf.float32)\n",
    "            # 透過dynamic_rnn對cell展開時間維度\n",
    "            self.encoder_outputs, self.encoder_state  = tf.nn.dynamic_rnn(\n",
    "                                                              encoder_cell, \n",
    "                                                              self.encoder_inputs,                                                    \n",
    "                                                              initial_state=self.initial_state\n",
    "                                                                          )\n",
    "\n",
    "            \n",
    "    # Decoder Model with Attention=========================================================\n",
    "    def build_decoder(self):\n",
    "        def get_a_cell(lstm_size, keep_prop):\n",
    "            lstm = tf.nn.rnn_cell.BasicLSTMCell(lstm_size)\n",
    "            drop = tf.nn.rnn_cell.DropoutWrapper(lstm, output_keep_prob=self.train_keep_prob)\n",
    "            return drop\n",
    "        \n",
    "        decoder_layer = [self.lstm_size[-1] for _ in range(4)]\n",
    "        d_cell = tf.nn.rnn_cell.MultiRNNCell(\n",
    "                                 [get_a_cell(size, self.keep_prob) for size in decoder_layer]\n",
    "                                            )\n",
    "        #d_cell = tf.nn.rnn_cell.BasicLSTMCell(self.lstm_size[-1])\n",
    "        #decoder_cell = tf.contrib.rnn.GRUCell(self.lstm_size[-1])\n",
    "        \n",
    "            \n",
    "        '''Wrappe decoder cell by attention mechanism'''\n",
    "        #attention_states: [batch_size, max_time, num_units]\n",
    "        attention_states = self.encoder_outputs\n",
    "        \n",
    "        attention_mechanism = tf.contrib.seq2seq.LuongAttention(\n",
    "                                 self.lstm_size[-1],\n",
    "                                 attention_states)\n",
    "        \n",
    "        decoder_cell = tf.contrib.seq2seq.AttentionWrapper(\n",
    "                                  d_cell,\n",
    "                                  attention_mechanism,\n",
    "                                  attention_layer_size=self.lstm_size[-1])\n",
    "        \n",
    "        #decoder_cell = tf.contrib.rnn.OutputProjectionWrapper(\n",
    "        #                          decoder_cell, \n",
    "        #                          self.output_len, \n",
    "        #                          reuse=tf.AUTO_REUSE\n",
    "        #                          )\n",
    "       \n",
    "        '''Project layer (output layer / full connecting layers)'''\n",
    "        project_layer = layers_core.Dense(self.output_len, \n",
    "                                          kernel_initializer=tf.truncated_normal_initializer(mean=0.1,stddev=0.1), \n",
    "                                          name=\"output_projection\") \n",
    "        \n",
    "        decoder_initial_state = decoder_cell.zero_state(batch_size, dtype=tf.float32)\n",
    "        \n",
    "        ''' Two decoder model:\n",
    "        . training_decoder : for training & target as input\n",
    "        . predict_decoder : for predecting & input by beam search etc...\n",
    "        '''\n",
    "        with tf.variable_scope(\"decode\", initializer=tf.orthogonal_initializer()):\n",
    "            '''1. Training decoder & output \n",
    "                  Time_major =False(default): [batch_size, max_seq_len, vector_len]\n",
    "                  Time_major =True : [max_seq_len, batch_size, vector_len]\n",
    "            '''\n",
    "            \n",
    "            training_helper = tf.contrib.seq2seq.TrainingHelper(\n",
    "                                                 self.decoder_inputs, \n",
    "                                                 self.batch_seq_len, \n",
    "                                                 time_major=False)\n",
    "            training_decoder = tf.contrib.seq2seq.BasicDecoder(\n",
    "                                                  cell=decoder_cell,\n",
    "                                                  helper=training_helper,\n",
    "                                                  initial_state=decoder_initial_state,#self.encoder_state[-1],\n",
    "                                                  output_layer=project_layer)\n",
    "            \n",
    "            train_outputs, _, _ = tf.contrib.seq2seq.dynamic_decode(training_decoder, impute_finished=True)\n",
    "            self.logits = train_outputs.rnn_output\n",
    "            self.train_prediction = tf.sigmoid(self.logits, name='train_predictions')\n",
    "            t_pred = tf.identity(self.train_prediction, name='t_pred')\n",
    "        \n",
    "        \n",
    "        with tf.variable_scope(\"decode\", reuse=True):\n",
    "            '''2. Predicting decoder & output (same parameter) '''\n",
    "            \n",
    "            '''Three function for CustomHelper'''\n",
    "            def initial_fn():\n",
    "                initial_elements_finished = self.go_token\n",
    "                initial_input = tf.concat(self.go_token, shape=[self.batch_size, self.output_len])\n",
    "                return initial_elements_finished, initial_input\n",
    "\n",
    "            def sample_fn(time, outputs, state):\n",
    "                # 选择logit最大的下标作为sample\n",
    "                prediction = tf.to_int32(outputs)\n",
    "                return prediction\n",
    "\n",
    "            def next_inputs_fn(time, outputs, state, sample_ids):\n",
    "                next_input = tf.concat((outputs, encoder_outputs[time]), 1)\n",
    "                elements_finished = (time >= decoder_lengths)  # this operation produces boolean tensor of [batch_size]\n",
    "                next_state = state\n",
    "                return elements_finished, next_inputs, next_state\n",
    "        \n",
    "        \n",
    "            predicting_helper = tf.contrib.seq2seq.CustomHelper(initial_fn, sample_fn, next_inputs_fn)\n",
    "\n",
    "            predicting_decoder = tf.contrib.seq2seq.BasicDecoder(cell=decoder_cell,\n",
    "                                                                 helper=predicting_helper,\n",
    "                                                                 initial_state=decoder_initial_state,\n",
    "                                                                 output_layer=project_layer)\n",
    "            predicting_outputs, _, _ = tf.contrib.seq2seq.dynamic_decode(training_decoder, impute_finished=True)\n",
    "            predict_logits = train_outputs.rnn_output\n",
    "            self.final_prediction = tf.sigmoid(predict_logits, name='model_predictions')\n",
    "            model_pred = tf.identity(self.final_prediction, name='model_pred')\n",
    "        \n",
    "        \n",
    "\n",
    "    # Loss & Optimizer ==============================================================================\n",
    "    def build_loss(self):\n",
    "        with tf.name_scope('loss'):\n",
    "            #self.y_reshaped = tf.reshape(self.targets,  self.logits.get_shape())\n",
    "            #self.loss =tf.losses.mean_squared_error(predictions=self.logits, labels=self.targets)\n",
    "            loss = tf.nn.sigmoid_cross_entropy_with_logits(logits=self.logits, labels=self.targets)\n",
    "            self.loss = tf.reduce_mean(loss)\n",
    "\n",
    "    def build_optimizer(self):\n",
    "        # Using \"clipping\" gradients\n",
    "        tvars = tf.trainable_variables()\n",
    "        grads, _ = tf.clip_by_global_norm(tf.gradients(self.loss, tvars), self.grad_clip)\n",
    "        train_op = tf.train.AdamOptimizer(self.learning_rate)\n",
    "        self.optimizer = train_op.apply_gradients(zip(grads, tvars))  \n",
    "        \n",
    "        \n",
    "\n",
    "    # Training===============================================================================    \n",
    "    def train(self, x, y, iters=10,  save_every_n=200, log_every_n=200):\n",
    "        self.train_graph = tf.Graph()\n",
    "\n",
    "        with tf.Session() as sess:\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "            sess.run(self.initial_state)           \n",
    "            \n",
    "            for ite in range(iters):\n",
    "                step = 0\n",
    "                print('iters: {}'.format(ite))\n",
    "                for i in range(len(x)):\n",
    "                    step += 1\n",
    "                    start = time.time()\n",
    "                    \n",
    "                    feed = {self.encoder_inputs: x[i], \n",
    "                            self.targets: y[i],\n",
    "                            self.decoder_inputs : y[i],\n",
    "                            self.keep_prob: self.train_keep_prob}\n",
    "                            #self.initial_state: new_state}\n",
    "                    \n",
    "                    batch_loss, new_state, pred, target = sess.run([self.loss,\n",
    "                                                      self.optimizer,\n",
    "                                                      self.train_prediction,\n",
    "                                                      self.targets],\n",
    "                                                      feed_dict=feed)\n",
    "                    end = time.time()\n",
    "                    \n",
    "                    # control the print lines\n",
    "                    if step % log_every_n == 0:\n",
    "                        print(\"=======================================================\\n\")\n",
    "                        print('step: {} in iter: {}/{}... '.format(step, ite+1, iters),\n",
    "                              'loss: {:.15f}... '.format(batch_loss),\n",
    "                              '{:.4f} sec/batch'.format((end - start)))\n",
    "\n",
    "                    if (step % save_every_n == 0):\n",
    "                        self.saver.save(sess, './seq2seq_(y_to_x)/model')\n",
    "    \n",
    "    \n",
    "    def predict(self, x, y, variable_name='model_pred'):\n",
    "        with tf.Session() as sess:\n",
    "            loader = tf.train.import_meta_graph('./seq2seq_(y_to_x)/model.meta')\n",
    "            loader.restore(sess, './seq2seq_(y_to_x)/model')\n",
    "            graph = tf.get_default_graph()\n",
    "            \n",
    "            '''Get the tensor'''\n",
    "            encoder_input = graph.get_tensor_by_name('inputs:0')\n",
    "            decoder_input = graph.get_tensor_by_name('decoder_inputs:0')\n",
    "            target = graph.get_tensor_by_name('targets:0')\n",
    "            keep_prob= graph.get_tensor_by_name('keep_prob:0')\n",
    "            '''Note the same name will be added \"_1\" with default'''\n",
    "            prediction = sess.graph.get_tensor_by_name('decode_1/model_pred:0')\n",
    "            \n",
    "            self.count = 0\n",
    "            self.total = 0\n",
    "            \n",
    "            for i in range(len(x)):\n",
    "                feed = {self.encoder_inputs: x[i], \n",
    "                                self.targets: y[0], # just make sure target not influence result\n",
    "                                self.decoder_inputs : y[0], # just make sure target not influence result\n",
    "                                self.keep_prob: 1.}\n",
    "                self.answer = sess.run(prediction, feed_dict=feed)\n",
    "                c, t = self.accuracy(self.answer, y[i])\n",
    "                self.count += c\n",
    "                self.total += t\n",
    "        return self.count/self.total\n",
    "            # print(answer)\n",
    "        \n",
    "    \n",
    "    def accuracy(self, pred, target):\n",
    "        pred = np.array(pred)\n",
    "        pred = np.array(pred >= 0.5).astype(int)\n",
    "        result = np.abs(pred - target)\n",
    "        \n",
    "        count = np.sum(result)\n",
    "        total = result.size\n",
    "\n",
    "        for i in range(result.shape[0]):\n",
    "            np.savetxt(\"result_\" + str(i) + \".csv\", result[i], delimiter=',')\n",
    "        return count, total\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nseq_max_len=1.,  \\ninput_len=41.,\\noutput_len=71.,\\nbatch_size=1,\\nlstm_size=[128., 128., 128.],\\nlearning_rate=0.005,\\ngrad_clip=2.,\\nkeep_prob=0.8,\\nforward_only= None\\n'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "seq_max_len=1.,  \n",
    "input_len=41.,\n",
    "output_len=71.,\n",
    "batch_size=1,\n",
    "lstm_size=[128., 128., 128.],\n",
    "learning_rate=0.005,\n",
    "grad_clip=2.,\n",
    "keep_prob=0.8,\n",
    "forward_only= None\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1\n",
    "seq_len = 1\n",
    "lstm_size=[70., 70., 70.]\n",
    "learning_rate=0.1\n",
    "keep_prob=0.005\n",
    "iters=1\n",
    "\n",
    "save_path='./seq2seq_(y_to_x)'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unuse data 0\n",
      "Train Batch: 47700 ; Test Batch: 5299\n",
      "iters: 0\n",
      "=======================================================\n",
      "\n",
      "step: 200 in iter: 1/1...  loss: 0.765205025672913...  0.0059 sec/batch\n",
      "=======================================================\n",
      "\n",
      "step: 400 in iter: 1/1...  loss: 0.687609374523163...  0.0058 sec/batch\n",
      "=======================================================\n",
      "\n",
      "step: 600 in iter: 1/1...  loss: 0.682296693325043...  0.0058 sec/batch\n",
      "=======================================================\n",
      "\n",
      "step: 800 in iter: 1/1...  loss: 0.726226687431335...  0.0058 sec/batch\n",
      "=======================================================\n",
      "\n",
      "step: 1000 in iter: 1/1...  loss: 0.711045563220978...  0.0058 sec/batch\n",
      "=======================================================\n",
      "\n",
      "step: 1200 in iter: 1/1...  loss: 0.705324172973633...  0.0057 sec/batch\n",
      "=======================================================\n",
      "\n",
      "step: 1400 in iter: 1/1...  loss: 0.708758771419525...  0.0057 sec/batch\n",
      "=======================================================\n",
      "\n",
      "step: 1600 in iter: 1/1...  loss: 0.728832960128784...  0.0057 sec/batch\n",
      "=======================================================\n",
      "\n",
      "step: 1800 in iter: 1/1...  loss: 0.721849620342255...  0.0057 sec/batch\n",
      "=======================================================\n",
      "\n",
      "step: 2000 in iter: 1/1...  loss: 0.701219141483307...  0.0057 sec/batch\n",
      "=======================================================\n",
      "\n",
      "step: 2200 in iter: 1/1...  loss: 0.697612166404724...  0.0058 sec/batch\n",
      "=======================================================\n",
      "\n",
      "step: 2400 in iter: 1/1...  loss: 0.698890984058380...  0.0066 sec/batch\n",
      "=======================================================\n",
      "\n",
      "step: 2600 in iter: 1/1...  loss: 0.728691279888153...  0.0058 sec/batch\n",
      "=======================================================\n",
      "\n",
      "step: 2800 in iter: 1/1...  loss: 0.723160326480865...  0.0089 sec/batch\n",
      "=======================================================\n",
      "\n",
      "step: 3000 in iter: 1/1...  loss: 0.717485129833221...  0.0118 sec/batch\n",
      "=======================================================\n",
      "\n",
      "step: 3200 in iter: 1/1...  loss: 0.702111780643463...  0.0059 sec/batch\n",
      "=======================================================\n",
      "\n",
      "step: 3400 in iter: 1/1...  loss: 0.716592609882355...  0.0057 sec/batch\n",
      "=======================================================\n",
      "\n",
      "step: 3600 in iter: 1/1...  loss: 0.721141815185547...  0.0116 sec/batch\n",
      "=======================================================\n",
      "\n",
      "step: 3800 in iter: 1/1...  loss: 0.715566396713257...  0.0057 sec/batch\n",
      "=======================================================\n",
      "\n",
      "step: 4000 in iter: 1/1...  loss: 0.694713532924652...  0.0057 sec/batch\n",
      "=======================================================\n",
      "\n",
      "step: 4200 in iter: 1/1...  loss: 0.688049912452698...  0.0055 sec/batch\n",
      "=======================================================\n",
      "\n",
      "step: 4400 in iter: 1/1...  loss: 0.675008416175842...  0.0057 sec/batch\n",
      "=======================================================\n",
      "\n",
      "step: 4600 in iter: 1/1...  loss: 0.744044363498688...  0.0057 sec/batch\n",
      "=======================================================\n",
      "\n",
      "step: 4800 in iter: 1/1...  loss: 0.705959618091583...  0.0057 sec/batch\n",
      "=======================================================\n",
      "\n",
      "step: 5000 in iter: 1/1...  loss: 0.691498458385468...  0.0067 sec/batch\n",
      "=======================================================\n",
      "\n",
      "step: 5200 in iter: 1/1...  loss: 0.709567189216614...  0.0057 sec/batch\n",
      "=======================================================\n",
      "\n",
      "step: 5400 in iter: 1/1...  loss: 0.702296376228333...  0.0058 sec/batch\n",
      "=======================================================\n",
      "\n",
      "step: 5600 in iter: 1/1...  loss: 0.705843091011047...  0.0057 sec/batch\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-12f8bc7a7960>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m             \u001b[0miters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0miters\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m             \u001b[0msave_every_n\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m             \u001b[0mlog_every_n\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m             )\n",
      "\u001b[0;32m<ipython-input-7-4a2137966fef>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, x, y, iters, save_every_n, log_every_n)\u001b[0m\n\u001b[1;32m    220\u001b[0m                                                       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_prediction\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m                                                       self.targets],\n\u001b[0;32m--> 222\u001b[0;31m                                                       feed_dict=feed)\n\u001b[0m\u001b[1;32m    223\u001b[0m                     \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf_gpu/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    903\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    904\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 905\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    906\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    907\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf_gpu/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1135\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1136\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1137\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1138\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1139\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf_gpu/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1353\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1354\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1355\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1356\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1357\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf_gpu/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1359\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1360\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1361\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1362\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1363\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf_gpu/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1338\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1339\u001b[0m           return tf_session.TF_Run(session, options, feed_dict, fetch_list,\n\u001b[0;32m-> 1340\u001b[0;31m                                    target_list, status, run_metadata)\n\u001b[0m\u001b[1;32m   1341\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1342\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if os.path.exists(save_path) is False:\n",
    "    os.makedirs(save_path)\n",
    "    \n",
    "train_input, train_target, test_input, test_target = batch_generator(\n",
    "                                                                x_input,\n",
    "                                                                y_label,\n",
    "                                                                batch_size=batch_size, \n",
    "                                                                seq_len=seq_len)\n",
    "\n",
    "model = Seq2Seq(batch_size=batch_size, \n",
    "                seq_max_len=seq_len,\n",
    "                lstm_size=lstm_size,\n",
    "                learning_rate=learning_rate)\n",
    "\n",
    "model.train(train_input, \n",
    "            train_target,\n",
    "            iters=iters,\n",
    "            save_every_n=1000,\n",
    "            log_every_n =200\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./seq2seq_(y_to_x)/model\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.49845705673937946"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(test_input, test_target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "eq2seq的NMT模型怎样训练-论文《Massive Exploration of Neural Machine Translation Architectures》\n",
    "http://cairohy.github.io/2017/04/11/deeplearning/NLP-Hyperparams-train-arXiv2017-%E3%80%8AMassive%20Exploration%20of%20Neural%20Machine%20Translation%20Architectures%E3%80%8B/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
