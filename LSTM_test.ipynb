{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time \n",
    "import numpy as np\n",
    "from __future__ import print_function\n",
    "\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "class N2N_LSTM:\n",
    "    def __init__(self,\n",
    "                 seq_max_len=10,  \n",
    "                 input_len=40,\n",
    "                 output_len=40,\n",
    "                 batch_size=1,\n",
    "                 lstm_size=[40, 80, 80, 80, 40],\n",
    "                 num_layers=5,\n",
    "                 learning_rate=0.001,\n",
    "                 grad_clip=5,\n",
    "                 keep_prob=1.,\n",
    "                 ):\n",
    "            \n",
    "        self.seq_max_len = seq_max_len\n",
    "        self.input_len = input_len\n",
    "        self.output_len = output_len\n",
    "        self.batch_size = batch_size\n",
    "        self.lstm_size = lstm_size\n",
    "        self.num_layers = num_layers\n",
    "        self.learning_rate = learning_rate\n",
    "        self.grad_clip = grad_clip\n",
    "        self.train_keep_prob = keep_prob\n",
    "        \n",
    "        tf.reset_default_graph() #Clears the default graph stack and resets the global default graph\n",
    "        self.build_inputs()\n",
    "        self.build_lstm()\n",
    "        self.build_loss()\n",
    "        self.build_optimizer()\n",
    "        self.saver = tf.train.Saver() #Saves and restores variables.\n",
    "        \n",
    "    \n",
    "    def build_inputs(self):\n",
    "        with tf.name_scope('inputs'):\n",
    "            self.inputs = tf.placeholder(tf.float32, \n",
    "                                         shape=(self.batch_size, self.seq_max_len, self.input_len),\n",
    "                                         name='inputs')\n",
    "            self.targets = tf.placeholder(tf.float32,\n",
    "                                          shape=(self.batch_size, self.seq_max_len, self.output_len),\n",
    "                                          name='targets')\n",
    "            self.keep_prob = tf.placeholder(tf.float32, name='keep_prob')\n",
    "            \n",
    "            \n",
    "            \n",
    "    def build_lstm(self):\n",
    "        def get_a_cell(lstm_size, keep_prop):\n",
    "            lstm = tf.nn.rnn_cell.BasicLSTMCell(lstm_size)\n",
    "            drop = tf.nn.rnn_cell.DropoutWrapper(lstm, output_keep_prob=self.train_keep_prob)\n",
    "            return drop\n",
    "        \n",
    "        with tf.name_scope('lstm'):\n",
    "            cell = tf.nn.rnn_cell.MultiRNNCell(\n",
    "                         [get_a_cell(size, self.keep_prob) for size in self.lstm_size]\n",
    "                                              )\n",
    "            self.initial_state = cell.zero_state(self.batch_size, tf.float32)\n",
    "            \n",
    "            # 透過dynamic_rnn對cell展開時間維度\n",
    "            self.lstm_outputs, self.final_state = tf.nn.dynamic_rnn(cell, \n",
    "                                                                    self.inputs, \n",
    "                                                                    initial_state=self.initial_state)\n",
    "            \n",
    "            # self.sigmoid_outputs = tf.nn.sigmoid(self.lstm_outputs, name='sigmoid')\n",
    "            \n",
    "            '''\n",
    "            # 透過lstm_outputs得到機率\n",
    "            seq_output = tf.concat(self.lstm_outputs, 1)\n",
    "            x = tf.reshape(seq_output, [-1, self.lstm_size[-1]])\n",
    "\n",
    "            with tf.variable_scope('sigmoid'):\n",
    "                sigmoid_w = tf.Variable(tf.truncated_normal([self.lstm_size[-1],self.output_len], stddev=0.1))\n",
    "                sigmoid_b = tf.Variable(tf.zeros(self.output_len))\n",
    "\n",
    "            self.logits = tf.matmul(x, sigmoid_w) + sigmoid_b\n",
    "            self.proba_prediction = tf.nn.sigmoid(self.logits, name='predictions')\n",
    "            '''\n",
    "            \n",
    "            \n",
    "    def build_loss(self):\n",
    "        with tf.name_scope('loss'):\n",
    "            y_reshaped = tf.reshape(self.targets, self.lstm_outputs.get_shape())\n",
    "            loss = tf.nn.sigmoid_cross_entropy_with_logits(logits=self.lstm_outputs, labels=y_reshaped)\n",
    "            self.loss = tf.reduce_mean(loss)\n",
    "            \n",
    "            \n",
    "\n",
    "    def build_optimizer(self):\n",
    "        # 使用clipping gradients\n",
    "        tvars = tf.trainable_variables()\n",
    "        grads, _ = tf.clip_by_global_norm(tf.gradients(self.loss, tvars), self.grad_clip)\n",
    "        train_op = tf.train.AdamOptimizer(self.learning_rate)\n",
    "        self.optimizer = train_op.apply_gradients(zip(grads, tvars))\n",
    "\n",
    "\n",
    "\n",
    "            \n",
    "\n",
    "    def train(self, batch_generator, iters=10, save_path='./models', save_every_n=200, log_every_n=50):\n",
    "        self.session = tf.Session()\n",
    "        with self.session as sess:\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "            # Train network\n",
    "            \n",
    "            new_state = sess.run(self.initial_state)\n",
    "            for ite in range(iters):\n",
    "                step = 0\n",
    "                print('iters',ite)\n",
    "                for x, y in batch_generator:\n",
    "                    step += 1\n",
    "                    start = time.time()\n",
    "                    feed = {self.inputs: x,\n",
    "                            self.targets: y,\n",
    "                            self.keep_prob: self.train_keep_prob,\n",
    "                            self.initial_state: new_state}\n",
    "                    batch_loss, new_state, _ = sess.run([self.loss,\n",
    "                                                         self.final_state,\n",
    "                                                         self.optimizer],\n",
    "                                                        feed_dict=feed)\n",
    "\n",
    "                    end = time.time()\n",
    "\n",
    "                    # control the print lines\n",
    "                    if step % log_every_n == 0:\n",
    "                        print('step: {} in iter: {}/{}... '.format(step, ite, iters),\n",
    "                              'loss: {:.4f}... '.format(batch_loss),\n",
    "                              '{:.4f} sec/batch'.format((end - start)))\n",
    "                    if (step % save_every_n == 0):\n",
    "                        self.saver.save(sess, os.path.join(save_path, 'model'), global_step=step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iters 0\n",
      "step: 10 in iter: 0/10...  loss: 0.6933...  0.0157 sec/batch\n",
      "step: 20 in iter: 0/10...  loss: 0.6929...  0.0151 sec/batch\n",
      "step: 30 in iter: 0/10...  loss: 0.6930...  0.0236 sec/batch\n",
      "step: 40 in iter: 0/10...  loss: 0.6926...  0.0158 sec/batch\n",
      "step: 50 in iter: 0/10...  loss: 0.6941...  0.0154 sec/batch\n",
      "step: 60 in iter: 0/10...  loss: 0.6940...  0.0160 sec/batch\n",
      "step: 70 in iter: 0/10...  loss: 0.6943...  0.0155 sec/batch\n",
      "step: 80 in iter: 0/10...  loss: 0.6931...  0.0174 sec/batch\n",
      "step: 90 in iter: 0/10...  loss: 0.6930...  0.0152 sec/batch\n",
      "step: 100 in iter: 0/10...  loss: 0.6928...  0.0148 sec/batch\n",
      "step: 110 in iter: 0/10...  loss: 0.6935...  0.0147 sec/batch\n",
      "step: 120 in iter: 0/10...  loss: 0.6927...  0.0149 sec/batch\n",
      "step: 130 in iter: 0/10...  loss: 0.6926...  0.0146 sec/batch\n",
      "step: 140 in iter: 0/10...  loss: 0.6924...  0.0157 sec/batch\n",
      "step: 150 in iter: 0/10...  loss: 0.6930...  0.0202 sec/batch\n",
      "step: 160 in iter: 0/10...  loss: 0.6924...  0.0153 sec/batch\n",
      "step: 170 in iter: 0/10...  loss: 0.6928...  0.0149 sec/batch\n",
      "step: 180 in iter: 0/10...  loss: 0.6928...  0.0159 sec/batch\n",
      "step: 190 in iter: 0/10...  loss: 0.6921...  0.0142 sec/batch\n",
      "step: 200 in iter: 0/10...  loss: 0.6934...  0.0154 sec/batch\n",
      "step: 210 in iter: 0/10...  loss: 0.6939...  0.0147 sec/batch\n",
      "step: 220 in iter: 0/10...  loss: 0.6932...  0.0342 sec/batch\n",
      "step: 230 in iter: 0/10...  loss: 0.6928...  0.0156 sec/batch\n",
      "step: 240 in iter: 0/10...  loss: 0.6934...  0.0149 sec/batch\n",
      "step: 250 in iter: 0/10...  loss: 0.6934...  0.0147 sec/batch\n",
      "step: 260 in iter: 0/10...  loss: 0.6930...  0.0153 sec/batch\n",
      "step: 270 in iter: 0/10...  loss: 0.6934...  0.0148 sec/batch\n",
      "step: 280 in iter: 0/10...  loss: 0.6926...  0.0319 sec/batch\n",
      "step: 290 in iter: 0/10...  loss: 0.6937...  0.0184 sec/batch\n",
      "step: 300 in iter: 0/10...  loss: 0.6927...  0.0147 sec/batch\n",
      "iters 1\n",
      "iters 2\n",
      "iters 3\n",
      "iters 4\n",
      "iters 5\n",
      "iters 6\n",
      "iters 7\n",
      "iters 8\n",
      "iters 9\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    model_path = './models'\n",
    "    if os.path.exists(model_path) is False:\n",
    "        os.makedirs(model_path)\n",
    "    g = batch_generator()\n",
    "    model = N2N_LSTM()\n",
    "    model.train(g,\n",
    "                iters=10,\n",
    "                save_path=model_path,\n",
    "                save_every_n=200,\n",
    "                log_every_n =10\n",
    "                )\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# return: batch_generator\n",
    "#g = batch_generator(arr, FLAGS.num_seqs, FLAGS.num_steps)\n",
    "\n",
    "def batch_generator(input_len=40, output_len=40, batch_size=1, total=3000, seq_len=10):\n",
    "    runs = int(total/seq_len/batch_size)\n",
    "    \n",
    "    for run in range(runs):\n",
    "        x = np.zeros(shape=(batch_size, seq_len, input_len))\n",
    "        y = np.zeros(shape=(batch_size, seq_len, output_len))\n",
    "        for seq in range(seq_len):\n",
    "            for batch in range(batch_size):\n",
    "                sequen = np.random.randint(2, size=input_len)\n",
    "                x[batch, seq, :] = sequen\n",
    "                y[batch, seq, :] = np.abs(sequen - 1)\n",
    "        yield (x, y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
